{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn import maskers, image\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurst exponent function definition:\n",
    "    R/S method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hurst import compute_Hc \n",
    "\n",
    "def calculate_hurst(time_series):\n",
    "    \"\"\"\n",
    "    Calculate the Hurst exponent for each voxel in the input 2D array.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: 2D array where each column is a time series of a specific voxel.\n",
    "\n",
    "    Returns:\n",
    "    - Array of Hurst exponents for each voxels.\n",
    "    \"\"\"\n",
    "\n",
    "    hurst_exponent=0.0\n",
    "    hurst_constant=0.0\n",
    "    hurst_data=[]\n",
    "    # for column in zip(*time_series):\n",
    "    \n",
    "    if(len(time_series) > 111):\n",
    "        hurst_exponent, hurst_constant, _ = compute_Hc(time_series)\n",
    "    return hurst_exponent,hurst_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurst exponent function definition:\n",
    "    DFA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_window_sizes(series_length):\n",
    "    if series_length < 112:\n",
    "        window_sizes = [0]\n",
    "    else:\n",
    "        scale_lim=[3,5]\n",
    "        scale_dens=0.20\n",
    "        window_sizes=(2**np.arange(scale_lim[0], scale_lim[1], scale_dens)).astype(int)\n",
    "    return window_sizes\n",
    "\n",
    "\n",
    "def calc_rms(x, scale):\n",
    "    shape = (x.shape[0] // scale, scale)\n",
    "    X = np.lib.stride_tricks.as_strided(x, shape=shape)\n",
    "    scale_ax = np.arange(scale)\n",
    "    rms = np.zeros(X.shape[0])\n",
    "    for e, xcut in enumerate(X):\n",
    "        coeff = np.polyfit(scale_ax, xcut, 1)\n",
    "        xfit = np.polyval(coeff, scale_ax)\n",
    "        rms[e] = np.sqrt(np.mean((xcut - xfit)**2))\n",
    "    return rms\n",
    "\n",
    "def dfa( time_series_data,window_sizes,show=False):\n",
    "    if len(window_sizes) != 1 :\n",
    "        y = np.cumsum(time_series_data - np.mean(time_series_data))\n",
    "        fluct = np.zeros(len(window_sizes))\n",
    "        for e, sc in enumerate(window_sizes):\n",
    "            fluct[e] = np.mean(np.sqrt(calc_rms(y, sc)**2))\n",
    "        coeff = np.polyfit(np.log2(window_sizes), np.log2(fluct), 1)\n",
    "        hurst_exponent = coeff[0]\n",
    "        # Calculate R-squared\n",
    "        predicted_values = np.polyval(coeff, np.log2(window_sizes))\n",
    "        mean_fluct = np.mean(np.log2(fluct))\n",
    "        tss = np.sum((np.log2(fluct) - mean_fluct) ** 2)\n",
    "        rss = np.sum((np.log2(fluct) - predicted_values) ** 2)\n",
    "        r_squared = 1 - (rss / tss)\n",
    "\n",
    "        if show:\n",
    "            fluctfit = 2**np.polyval(coeff, np.log2(window_sizes))\n",
    "            plt.loglog(window_sizes, fluct, 'bo')\n",
    "            plt.loglog(window_sizes, fluctfit, 'r', label=f'H = {coeff[0]:0.2f}, RÂ² = {r_squared:.2f}')\n",
    "            plt.title('Detrended Fluctuation Analysis')\n",
    "            plt.xlabel(r'$\\log_{2}$(time window)')\n",
    "            plt.ylabel(r'$\\log_{2}$<F(t)>')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        hurst_exponent=0\n",
    "        r_squared=0\n",
    "    \n",
    "    return hurst_exponent, r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_step_size(length, window_size):\n",
    "    num_windows = math.ceil(length / window_size)\n",
    "    if num_windows == 1:\n",
    "        return window_size  # No overlap needed if the list is shorter than the window size\n",
    "    \n",
    "    step_size = max(1, (length - window_size) // (num_windows - 1))\n",
    "\n",
    "    return step_size\n",
    "\n",
    "\n",
    "def create_slices(data, window_size):\n",
    "    step_size = calculate_step_size(len(data), window_size)\n",
    "    windows = [data[i:i + window_size] for i in range(0, len(data) - window_size + 1, step_size)]\n",
    "    return windows\n",
    "\n",
    "def calculate_hurst_dfa(time_series,show=False):\n",
    "    hurst_exponents =[]\n",
    "    fit_scores =[]\n",
    "    window_size = 112\n",
    "    # column_slices = create_slices(time_series,window_size)\n",
    "\n",
    "    # if column_slices == []:\n",
    "    #     hurst_exponents.append(0)\n",
    "    #     fit_scores.append(0)\n",
    "    # else:\n",
    "        # for i, window in enumerate(column_slices):\n",
    "    hurst_value= 0\n",
    "    window_sizes = calculate_window_sizes(len(time_series))\n",
    "    hurst_value,r_squared = dfa(time_series,window_sizes,show=True)\n",
    "    # if(r_squared < 0.95):\n",
    "    #     hurst_value = 0\n",
    "    hurst_exponents.append(hurst_value)\n",
    "    fit_scores.append(r_squared)\n",
    "    return hurst_exponents,fit_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example DFA Hurst Exponent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Usage:\n",
    "time_series_data = [-0.0017928385431826186, 0.12523051095819668, 0.15675004725212976, 0.11124722197605776, 0.06990221378029653, 0.05371896659605649, 0.024071357592601234, -0.010131349614161938, 0.027633408068171668, 0.16165649921465566, 0.28542888801566957, 0.262034976554124, 0.08193245613209767, -0.12581938265680126, -0.2279459358019513, -0.19424932138447076, -0.08667456487314125, 0.00875985737537579, 0.03496962384090565, -0.00992013560439811, -0.07475316319179794, -0.0930233134453674, -0.03549503935461722, 0.06357033812676069, 0.1296659269126457, 0.10529157208049339, -0.002489661478363574, -0.12392538400829951, -0.18032232658485622, -0.14637810138174698, -0.06461113435812615, 0.0008813169932818818, 0.02209112573944713, 0.01885805489348043, 0.02213245308107517, 0.036638599583005575, 0.04213377993743712, 0.022868800786330325, -0.013567652207144006, -0.04679865806059885, -0.06756828478953678, -0.08652234093326636, -0.11382241863945324, -0.12937270928903766, -0.0888791592841364, 0.02141192147811009, 0.14294473262281288, 0.1857691116327778, 0.12255890506125997, 0.026547837197958654, 0.0019794775973665784, 0.07945677174202927, 0.1856881807003871, 0.21541148214597003, 0.132594581695328, 0.0054433872294189205, -0.05803717493443957, -0.01743612623419404, 0.05510403231313167, 0.04903735374510066, -0.06278672907669842, -0.1934172864174408, -0.2392237404531362, -0.18718422655059097, -0.10917196457732985, -0.06520036216963121, -0.04022529428235227, 0.014658434425865749, 0.0981797258429603, 0.1472349241018424, 0.11686308386040548, 0.040630565025067227, -0.016853350196586285, -0.045818055028717664, -0.0873657820449646, -0.1442197069749069, -0.14211025372739683, -0.020844694704800725, 0.14673808926437473, 0.1863144199452584, 0.019051726332984395, -0.20451599848581586, -0.2433762574532787, -0.04767778723420615, 0.17225753073183814, 0.19027668087662858, 0.011405962037251282, -0.16950509724571383, -0.19591820826434503, -0.07564626812561454, 0.08312969836720303, 0.19092152796828407, 0.22650506018946695, 0.21073537582966648, 0.15819430964821773, 0.061543056835714614, -0.07056147065145364, -0.17601061799859252, -0.18071110686106803, -0.08611366945813145, 0.025339475364129435, 0.08608378006072419, 0.09986969596065941, 0.08519692520888547, 0.022262862105094372, -0.09898181389345735, -0.22488267238936768, -0.2890684431607644, -0.28771230908674905, -0.25072169855338644, -0.1659652078135314, 0.003097529992504004, 0.23073744925770517, 0.4145474264877708, 0.4567668862640695, 0.338969869813705, 0.13636354370739476, -0.013846343158207519, 0.000570326176883506, 0.14802003565849, 0.24125833559071502, 0.11878331884690728, -0.15621189444674352, -0.34339859809934, -0.28828877326163294, -0.08158075856772491, 0.0614832965208791, 0.02041183282282329, -0.16279738350794423, -0.3716994413668554, -0.503910011142099, -0.4991234108064798, -0.3552966161602029, -0.13645144835321402, 0.07211957502118807, 0.22851890675013548, 0.33556333946066136, 0.39053605738075675, 0.37442099209006413, 0.29478740654995256, 0.20342062179863207, 0.15494686022657084, 0.1622277763055538, 0.19559902338027005, 0.2105971901956893, 0.17572520789753315, 0.09026035395150932, -0.019635077702882395, -0.12958210690497524, -0.23520032100235994, -0.32840095568021077, -0.37138769707066166, -0.3318025061600793, -0.2469258145604436, -0.2063639425257068, -0.2385034162760304, -0.2476005878209344, -0.11663697141142366, 0.13241117608405975, 0.3305591013662113, 0.32949640877027864, 0.14914357244758852, -0.04890416781361836, -0.10812635705217796, 0.0046788235814435265, 0.191676711260044, 0.3114659688326604, 0.2862645597193996, 0.1467326753137744, -0.00958261417722743, -0.09757306380566776, -0.09400384995747946, -0.029151994171915153, 0.04924144794742821, 0.0936488347417316, 0.061855932291594376, -0.05078521685692597, -0.17519240525747637, -0.20782034658358767, -0.12103276321515428, -0.014853468979535722, -0.017102437370092413, -0.126824080658581, -0.1979628810502628, -0.11378093527430146, 0.04749364158422634, 0.08322776382514499, -0.08190980424295076, -0.27252821351916373, -0.24651083350016934, 0.01648461518809675, 0.2762146489779186, 0.30484349711875114, 0.1432726905568514, 0.024328118301811596, 0.07330744287820426, 0.18213497756370736, 0.19926903148955338, 0.14024761976139197, 0.12771346472633177, 0.18469420886372115, 0.20169233312005924, 0.10746806699315523, -0.0316851519058008, -0.1224147577764592, -0.17434529449744332, -0.2563175799280038, -0.3717707594818455, -0.4482604358421939, -0.4313530666636128, -0.3317388431871029, -0.1919984182981693, -0.05629472240547677, 0.0403743993497134, 0.10331206623544145, 0.19004196401522752, 0.3384537427100828, 0.47688153110409476, 0.4576847023502513, 0.21965883657804647, -0.10257392613639324, -0.2810330779047508, -0.2217935488444537, -0.05532225730168795, 0.023956204598837398, -0.02568345101197211, -0.07905644612800879, -0.0265393117558892, 0.09215957815699363, 0.1600140890947295, 0.13686361678134187, 0.08049272751844105, 0.03319728040709229, -0.030570227233554428, -0.11626822018784308, -0.1410963387075328, -0.02597966854469792, 0.16451274390080592, 0.24810318346860166, 0.11952884919729477, -0.11786967917420503, -0.2503447695725205, -0.1695627061166155, 0.03433443547085666, 0.17952841625702895, 0.15930344842973543, 0.008917099077916308, -0.1537019723755309, -0.23501703834578772, -0.2285778306238155, -0.18914880200702575, -0.1590606461018535, -0.13463330490145647, -0.0955036274518545, -0.03504766325864384, 0.04065518497016666, 0.11346666986089336, 0.15117078764147868, 0.1348317585742261, 0.08714221571233999, 0.04564086598925277, 0.01657594611455037, -0.008847684944479798, -0.0016756926821048564, 0.08683172318964218, 0.23325981170890997, 0.31169497036244775, 0.20594497718998117, -0.04862736522222998, -0.27307563912967114, -0.3135581736877329, -0.18448533796664593, -0.028322712759915856, 0.04639723614080599, 0.05372847460023967, 0.06056906520827326, 0.07690548975738984, 0.05907657068685738, 0.0020599998124161147, -0.037876693205869144, -0.030731236033970155]\n",
    "\n",
    "# H, c, data = compute_Hc(time_series_data)\n",
    "hurst_value = calculate_hurst_dfa(time_series_data,True)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(time_series_data)\n",
    "plt.xlabel(\"Timepoints\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.grid(True)\n",
    "print(\"Hurst Exponent:\", hurst_value)\n",
    "print(\"Length of the time series:\", len(time_series_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power noise test for evaluating correctness of Hurst evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law_noise(n, alpha, var=1):\n",
    "    '''\n",
    "    Generale power law noise. \n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "      *n* : int\n",
    "        number of data points\n",
    "      *alpha* : float\n",
    "        DFA exponent\n",
    "      *var* = 1 : float\n",
    "        variance\n",
    "    Returns:\n",
    "    --------\n",
    "      *x* : numpy.array\n",
    "        generated noisy data with exponent *alpha*\n",
    "\n",
    "    Based on:\n",
    "    N. Jeremy Kasdin, Discrete simulation of power law noise (for\n",
    "    oscillator stability evaluation)\n",
    "    '''\n",
    "    # computing standard deviation from variance\n",
    "    stdev = np.sqrt(np.abs(var))\n",
    "    beta = 2*alpha-1\n",
    "    hfa = np.zeros(2*n)\n",
    "    hfa[0] = 1\n",
    "    for i in range(1,n):\n",
    "        hfa[i] = hfa[i-1] * (0.5*beta + (i-1))/i\n",
    "    # sample white noise\n",
    "    wfa = np.hstack((-stdev +2*stdev * np.random.rand(n), np.zeros(n)))\n",
    "    fh = np.fft.fft(hfa)\n",
    "    fw = np.fft.fft(wfa)\n",
    "    fh = fh[1:n+1]\n",
    "    fw = fw[1:n+1]\n",
    "    ftot = fh * fw\n",
    "    # matching the conventions of the Numerical Recipes\n",
    "    ftot = np.hstack((ftot, np.zeros(n-1)))\n",
    "    x = np.fft.ifft(ftot)    \n",
    "    return np.real(x[:n])\n",
    "\n",
    "\n",
    "\n",
    "n = 128\n",
    "dfa_alpha = 0.9\n",
    "time_series_data = power_law_noise(n, dfa_alpha)\n",
    "hurst_value = calculate_hurst_dfa(time_series_data)\n",
    "print(\"Hurst Exponent:\", hurst_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the preprocessed data with the following:\n",
    "    - atlas : no atlas\n",
    "    - masking : EPI (Echo Planar Imaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_filename = \"../output_directory/sub-01/func/sub-01_task-rest_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold_postproc_smooth.nii.gz\"\n",
    "\n",
    "# initialising a masker without atlas\n",
    "brain_masker = maskers.NiftiMasker(mask_strategy=\"whole-brain-template\")\n",
    "\n",
    "# Apply masker\n",
    "time_series_epi = brain_masker.fit_transform(fmri_filename,\n",
    "                                               confounds=None)\n",
    "\n",
    "print(\"Dimension of the fMRI signal (x,y,z,timestamps): \", nibabel.load(fmri_filename).shape)\n",
    "print(\"Dimension of the EPI time series (timestamps , voxels): \", time_series_epi.shape)\n",
    "\n",
    "# Plotting time series of a voxel\n",
    "voxel = 1\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.plot(time_series_epi[:,voxel])\n",
    "plt.title(\"Timeseriese of a voxel\")\n",
    "plt.xlabel(\"Time (s)\", fontsize = 10)\n",
    "plt.ylabel(\"BOLD signal\", fontsize= 10)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# print(time_series_epi)\n",
    "\n",
    "report = brain_masker.generate_report()\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the preprocessed data with the following:\n",
    "    - atlas : msdl (Multi Subject Dictionary Learning)\n",
    "    - masking : atlas + no confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_filename = \"Dataset/sub-01/func/sub-01_task-rest_run-1_bold.nii.gz\"\n",
    "# \"preprocessed_dataset/sub-025_ses-base_task-rest_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "data_dir = \"../python_scripts/resources/atlas\"\n",
    "\n",
    "#MSDL atlas:\n",
    "msdl_atlas = datasets.fetch_atlas_msdl(data_dir=data_dir)\n",
    "\n",
    "msdl_coords = msdl_atlas.region_coords\n",
    "n_regions = len(msdl_coords)\n",
    "\n",
    "print(f'MSDL has {n_regions} ROIs, part of the following networks:{np.unique(msdl_atlas.networks)}.')\n",
    "\n",
    "# initialising a masker\n",
    "\n",
    "msdl_masker = maskers.NiftiMapsMasker(\n",
    "    msdl_atlas.maps, resampling_target=\"data\", detrend=True)\n",
    "\n",
    "\n",
    "# Apply masker\n",
    "time_series_msdl = msdl_masker.fit_transform(fmri_filename,confounds=None)\n",
    "\n",
    "print(\"Dimension of the fMRI signal (x,y,z,timestamps): \", nibabel.load(fmri_filename).shape)\n",
    "print(\"Dimension of the msdl time series (timestamps , voxels): \", time_series_msdl.shape)\n",
    "\n",
    "# Plotting time series of a voxel\n",
    "voxel = 38\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.plot(time_series_msdl[:, voxel], marker = 'o')\n",
    "plt.title(f'Timeseriese of {msdl_atlas.networks[voxel]} region')\n",
    "plt.xlabel(\"Time (s)\", fontsize = 10)\n",
    "plt.ylabel(\"BOLD signal\", fontsize= 10)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#Hurst component analysis:\n",
    "hurst_exponents_msdl,hurst_constants_msdl,hurst_data_msdl = calculate_hurst(time_series_msdl)\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.plot(hurst_exponents_msdl, marker = 'o')\n",
    "plt.title('Hurst exponent of every voxels')\n",
    "plt.xlabel(\"Voxels\", fontsize = 10)\n",
    "plt.ylabel(\"Hurst exponent\", fontsize= 10)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "report = msdl_masker.generate_report()\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the repeatition time for the fmri signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preprocessed = image.load_img('../preprocessed_dataset/sub-025_ses-base_task-rest_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')\n",
    "img_raw = image.load_img(\"../Dataset/sub-01/func/sub-01_task-rest_run-1_bold.nii.gz\")\n",
    "\n",
    "tr_raw = img_raw.header.get_zooms()[-1]  # Get the TR_raw\n",
    "print(\"Repetition Time (TR_raw):\", tr_raw)\n",
    "print(img_raw.header.get_zooms())\n",
    "\n",
    "tr_preprocessed = img_preprocessed.header.get_zooms()[-1]  # Get the TR_preprocessed\n",
    "print(\"Repetition Time (TR_preprocessed):\", tr_preprocessed)\n",
    "print(img_preprocessed.header.get_zooms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the path of files with specific name structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(main_directory,file_name):\n",
    "    \"\"\"\n",
    "    Get the path of files for different subjects.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "      *main_directory* : string\n",
    "        location of the main directory having all the subjects.\n",
    "      *file_name* : string\n",
    "        end of the file name which you are looking for including the file type\n",
    "    Returns:\n",
    "    --------\n",
    "      *file_path* : dictionary\n",
    "        dictionary with keys as subject_numbers and values as respective file path.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Get a list of subject directories that start with \"sub-\"\n",
    "    subject_directories = sorted([d for d in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, d)) and d.startswith(\"sub-\")])\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    fmri_path = []\n",
    "    file_path = {}\n",
    "\n",
    "    # Loop through subject directories\n",
    "    for subject_dir in subject_directories:\n",
    "        fmri_path = []\n",
    "        subject_path = os.path.join(main_directory, subject_dir)\n",
    "        func_dir = os.path.join(subject_path, \"func\")\n",
    "\n",
    "        if os.path.exists(func_dir) and os.path.isdir(func_dir):\n",
    "            # Assuming fMRI files have a common pattern, such as '*.nii.gz' inside the \"func\" subdirectory\n",
    "            fmri_files = [f for f in os.listdir(func_dir) if f.endswith(file_name)]\n",
    "\n",
    "            # Loop through fMRI files\n",
    "            for fmri_file in fmri_files:\n",
    "                fmri_path.append(os.path.join(func_dir, fmri_file))\n",
    "            file_path[subject_dir]= sorted(np.array(fmri_path))\n",
    "\n",
    "    return(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract number of rest and sleep files of all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process a single TSV file\n",
    "def process_tsv(file_path):\n",
    "    # Read the TSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Extract subject and run information from the file name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    subject = file_name.split('-s')[0]\n",
    "\n",
    "    # Group by run and calculate max epoch time and NREM states\n",
    "    grouped_data = df.groupby(['session', 'epoch_start_time_sec']).agg({'30-sec_epoch_sleep_stage': 'max'}).reset_index()\n",
    "    # Separate sleep runs and rest runs\n",
    "    sleep_runs = grouped_data[grouped_data['session'].str.contains('sleep_run')]\n",
    "    rest_runs = grouped_data[grouped_data['session'].str.contains('rest_run')]\n",
    "\n",
    "    # Calculate max epoch time and associated sleep states for each sleep run\n",
    "    max_time_and_states_sleep = {}\n",
    "    for run in sleep_runs['session'].unique():\n",
    "        run_data = sleep_runs[sleep_runs['session'] == run]\n",
    "        max_epoch_time = run_data['epoch_start_time_sec'].max()\n",
    "        sleep_states = run_data.sort_values('epoch_start_time_sec')['30-sec_epoch_sleep_stage'].tolist()\n",
    "        max_time_and_states_sleep[run] = {'max_epoch_time': max_epoch_time, 'sleep_states': sleep_states}\n",
    "\n",
    "    # Calculate max epoch time and associated sleep states for each rest run\n",
    "    max_time_and_states_rest = {}\n",
    "    for run in rest_runs['session'].unique():\n",
    "        run_data = rest_runs[rest_runs['session'] == run]\n",
    "        max_epoch_time = run_data['epoch_start_time_sec'].max()\n",
    "        sleep_states = run_data.sort_values('epoch_start_time_sec')['30-sec_epoch_sleep_stage'].tolist()\n",
    "        max_time_and_states_rest[run] = {'max_epoch_time': max_epoch_time, 'sleep_states': sleep_states}\n",
    "\n",
    "    return subject, max_time_and_states_sleep, max_time_and_states_rest\n",
    "\n",
    "# Function to process all TSV files in a directory\n",
    "def process_directory(directory_path):\n",
    "    source_data = {}\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for file_name in sorted(os.listdir(directory_path)):\n",
    "        if file_name.endswith('.tsv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            subject, max_time_and_states_sleep, max_time_and_states_rest = process_tsv(file_path)\n",
    "\n",
    "            # Create or update the dictionary\n",
    "            if subject not in source_data:\n",
    "                source_data[subject] = {}\n",
    "            source_data[subject]['max_time_and_states_sleep'] = max_time_and_states_sleep\n",
    "            source_data[subject]['max_time_and_states_rest'] = max_time_and_states_rest\n",
    "\n",
    "    return source_data\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "directory_path = '../Dataset/sourcedata'\n",
    "result = process_directory(directory_path)\n",
    "\n",
    "save_to_csv(result, '../Dataset/source_data_summary.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to analyse the number of rest and sleep runs vs subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def analyze_runs(data):\n",
    "    # Count the number of rest and sleep runs per user\n",
    "    rest_runs_count = {}\n",
    "    sleep_runs_count = {}\n",
    "\n",
    "    for user, user_data in data.items():\n",
    "        rest_runs_count[user] = len(user_data.get('max_time_and_states_rest', {}))\n",
    "        sleep_runs_count[user] = len(user_data.get('max_time_and_states_sleep', {}))\n",
    "\n",
    "    # Plot bar graphs\n",
    "    plot_bar_graph(rest_runs_count, 'Number of rest sessions per subject', 'Number of Rest Sessions')\n",
    "    plot_bar_graph(sleep_runs_count, 'Number of sleep sessions per subject', 'Number of Sleep Sessions')\n",
    "\n",
    "    # Count the number of subjects with the same number of rest and sleep runs\n",
    "    count_dict_rest = count_subjects_same_runs(rest_runs_count, 'Rest Runs')\n",
    "    count_dict_sleep = count_subjects_same_runs(sleep_runs_count, 'Sleep Runs')\n",
    "\n",
    "    # Plot bar graphs for subjects with the same number of rest and sleep runs\n",
    "    plot_count_subjects_bar_graph(count_dict_rest, 'Rest Runs')\n",
    "    plot_count_subjects_bar_graph(count_dict_sleep, 'Sleep Runs')\n",
    "\n",
    "def plot_bar_graph(data, title, ylabel):\n",
    "    users = list(data.keys())\n",
    "    counts = list(data.values())\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(users, counts, color='skyblue')\n",
    "    plt.xlabel('Subjects')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45, ha='right')  \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def count_subjects_same_runs(data, run_type):\n",
    "    count_dict = {}\n",
    "    for count in set(data.values()):\n",
    "        subjects_with_count = [subject for subject, count_subject in data.items() if count_subject == count]\n",
    "        count_dict[count] = subjects_with_count\n",
    "\n",
    "    print(f\"\\nNumber of subjects with the same number of {run_type}:\")\n",
    "    for count, subjects in count_dict.items():\n",
    "        print(f\"{count} {run_type}: {len(subjects)} subjects - {subjects}\")\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "def plot_count_subjects_bar_graph(count_dict, run_type):\n",
    "    counts = list(count_dict.keys())\n",
    "    subjects_count = [len(subjects) for subjects in count_dict.values()]\n",
    "\n",
    "    x_ticks = np.arange(0,11,1) \n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(counts, subjects_count, color='skyblue')\n",
    "    plt.xlabel(f'Number of {run_type}')\n",
    "    plt.ylabel('Number of Subjects')\n",
    "    plt.title(f'Subjects with the Same Number of {run_type}')\n",
    "    plt.xticks(x_ticks)\n",
    "    plt.show()\n",
    "\n",
    "analyze_runs(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing epochs time of each subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_runs(data):\n",
    "    # Extract max epoch time for each rest and sleep run\n",
    "    max_epoch_time_rest = {}\n",
    "    max_epoch_time_sleep = {}\n",
    "    \n",
    "    for user, user_data in data.items():\n",
    "        max_epoch_time_rest[user] = [run_data['max_epoch_time'] for run_data in user_data.get('max_time_and_states_rest', {}).values()]\n",
    "        max_epoch_time_sleep[user] = [run_data['max_epoch_time'] for run_data in user_data.get('max_time_and_states_sleep', {}).values()]\n",
    "    \n",
    "    # Plot bar graphs for max epoch time for rest and sleep runs\n",
    "    plot_max_epoch_time_bar_graph(max_epoch_time_rest, 'Rest Runs', 'Max Epoch Time for Rest Runs')\n",
    "    plot_max_epoch_time_bar_graph(max_epoch_time_sleep, 'Sleep Runs', 'Max Epoch Time for Sleep Runs')\n",
    "\n",
    "    # Identify subjects with the same max epoch time for rest and sleep runs\n",
    "    count_subjects_same_max_time(max_epoch_time_rest, 'Rest Runs')\n",
    "    count_subjects_same_max_time(max_epoch_time_sleep, 'Sleep Runs')\n",
    "\n",
    "def plot_max_epoch_time_bar_graph(data, run_type, ylabel):\n",
    "    users = list(data.keys())\n",
    "    max_epoch_times = [np.mean(max_times) for max_times in data.values()]\n",
    "    \n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.bar(users, max_epoch_times, color='skyblue')\n",
    "    plt.xlabel('Users')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'{ylabel} for {run_type}')\n",
    "    plt.show()\n",
    "\n",
    "def count_subjects_same_max_time(data, run_type):\n",
    "    count_dict = {}\n",
    "    for max_time in set(np.concatenate(list(data.values()))):\n",
    "        subjects_with_max_time = [user for user, max_times in data.items() if max_time in max_times]\n",
    "        count_dict[max_time] = subjects_with_max_time\n",
    "\n",
    "    print(f\"\\nSubjects with the same max epoch time for {run_type}:\")\n",
    "    for max_time, subjects in count_dict.items():\n",
    "        print(f\"Max Epoch Time {run_type}: {max_time}, Total Subjects: {len(subjects)}\")\n",
    "\n",
    "# Assuming you already have the 'result' dictionary\n",
    "analyze_runs(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Framewise Displacement evaluation:\n",
    "    The code extracts all the \"desc-confounds_timeseries.tsv\" file locations per subjects\n",
    "        if it is less than threshold value (0.7), if not then it stores the value and the timestamp(which is the row number + 1)\n",
    "        Calculates the avg of FD values and check if that is less than the threshold as well.\n",
    "    The script finally gives a csv file with the Quality check considering Framewise displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_path = get_file_path(\"../output_directory\", \"desc-confounds_timeseries.tsv\")\n",
    "\n",
    "def quality_check_fd(confounds_files, avg_threshold,voxel_vol_threshold):\n",
    "    \"\"\"\n",
    "    The code runs through the framwise displacement values per runs of each subject, extracts FD values and checks:\n",
    "        if it is less than threshold_avg,threshold_voxel value (0.7), if not then it stores the value and the timestamp(which is the row number + 1)\n",
    "        Calculates the avg of FD values and check if that is less than the threshold as well.\n",
    "    Args:\n",
    "    -----\n",
    "      *confounds_files* : dictionary\n",
    "        dictionary with keys as subject_numbers and values as respective file path.\n",
    "\n",
    "      *avg_threshold* : float\n",
    "        threshold value of the average of the framewise displacement values\n",
    "\n",
    "      *voxel_vol_threshold* : float\n",
    "        threshold value of framewise displacement per voxel volume (Individual Repeatition time)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "      *quality_check_results* : dictionary\n",
    "        dictionary of the quality check of every run per subject based on framewise displacement\n",
    "    \"\"\" \n",
    "\n",
    "    quality_check_results = []\n",
    "    for subject, file_paths in confounds_files.items():\n",
    "        for file_path in file_paths:\n",
    "            subject_data = {\"Subject Number\": subject, \"File Name\": os.path.basename(file_path),\n",
    "                            \"FD values(2.3mm - 4.3mm)\": [],\"Exceeding Threshold(2.3mm - 4.3mm)\": 0,\"Exceeding Timepoints(2.3mm - 4.3mm)\": [], \n",
    "                            \"FD values(4.3mm - 6.3mm)\": [],\"Exceeding Threshold(4.3mm - 6.3mm)\": 0,\"Exceeding Timepoints(4.3mm - 6.3mm)\": [],\n",
    "                            \"FD values(>6.3mm)\": [],\"Exceeding Threshold(>6.3mm)\": 0,\"Exceeding Timepoints(>6.3mm)\": [],\n",
    "                            \"Average FD\": 0.0, \"Avg > threshold\": \"\"}\n",
    "\n",
    "            df = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "            fd_values = df['framewise_displacement'].tolist()\n",
    "            subject_data[\"Average FD\"] = sum(fd_values[1:]) / len(fd_values[1:])\n",
    "            if subject_data[\"Average FD\"]>avg_threshold:\n",
    "                subject_data[\"Avg > threshold\"] = \"YES\"\n",
    "            else:\n",
    "                subject_data[\"Avg > threshold\"] = \"NO\"\n",
    "\n",
    "            exceeding_timepoints = [i+1 for i, fd in enumerate(fd_values) if voxel_vol_threshold <fd < (voxel_vol_threshold+2)]\n",
    "            subject_data[\"Exceeding Timepoints(2.3mm - 4.3mm)\"].extend(exceeding_timepoints)\n",
    "            subject_data[\"Exceeding Threshold(2.3mm - 4.3mm)\"] += len(exceeding_timepoints)\n",
    "            fd_above_threshold = [fd for fd in fd_values if voxel_vol_threshold <fd < (voxel_vol_threshold+2)]\n",
    "            subject_data[\"FD values(2.3mm - 4.3mm)\"].extend(fd_above_threshold)\n",
    "\n",
    "            exceeding_timepoints = [i+1 for i, fd in enumerate(fd_values) if (voxel_vol_threshold+2) <fd < (voxel_vol_threshold+4)]\n",
    "            subject_data[\"Exceeding Timepoints(4.3mm - 6.3mm)\"].extend(exceeding_timepoints)\n",
    "            subject_data[\"Exceeding Threshold(4.3mm - 6.3mm)\"] += len(exceeding_timepoints)\n",
    "            fd_above_threshold = [fd for fd in fd_values if (voxel_vol_threshold+2) <fd < (voxel_vol_threshold+4)]\n",
    "            subject_data[\"FD values(4.3mm - 6.3mm)\"].extend(fd_above_threshold)\n",
    "\n",
    "            exceeding_timepoints = [i+1 for i, fd in enumerate(fd_values) if fd > (voxel_vol_threshold+4)]\n",
    "            subject_data[\"Exceeding Timepoints(>6.3mm)\"].extend(exceeding_timepoints)\n",
    "            subject_data[\"Exceeding Threshold(>6.3mm)\"] += len(exceeding_timepoints)\n",
    "            fd_above_threshold = [fd for fd in fd_values if fd > (voxel_vol_threshold+4)]\n",
    "            subject_data[\"FD values(>6.3mm)\"].extend(fd_above_threshold)\n",
    "\n",
    "            quality_check_results.append(subject_data.copy())  # Use copy to create a new dictionary for each subject\n",
    "\n",
    "    return quality_check_results\n",
    "\n",
    "\n",
    "avg_threshold_value = 0.5\n",
    "voxel_vol_threshold_value =2.3\n",
    "output_csv_file = \"../output_directory/qc_fd_report_3.csv\"\n",
    "\n",
    "qc_fd_results = quality_check_fd(confounds_path, avg_threshold_value,voxel_vol_threshold_value)\n",
    "\n",
    "df = pd.DataFrame(qc_fd_results)\n",
    "df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract the run number from the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_task_run_from_filename(filename):\n",
    "    pattern = r'task-(rest|sleep)_run-\\d+'\n",
    "    match = re.search(pattern, filename)\n",
    "    return match.group() if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Code for visualising the average framewise displacement per 30 second epoch wrt NREM state:\n",
    "    First extracts all the \"desc-confounds_timeseries.tsv\" file locations per subjects\n",
    "    Make a csv file with average FD values for 30 seconds epoch for every runs per subject\n",
    "    Add the sleep scores based on the runs per 30 seconds per subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ptitprince as plt\n",
    "\n",
    "def epoch_fd_avg(confounds_files, window_size_seconds=30):\n",
    "    \"\"\"\n",
    "    The code runs through the framwise displacement values per runs of each subject, extracts FD values and stores Average of framewise displacement for a window of 30 seconds each\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "      *confounds_files* : dictionary\n",
    "        dictionary with keys as subject_numbers and values as respective file path.\n",
    "\n",
    "      *window_size_seconds* : int\n",
    "        window size in seconds: default of 30 seconds epochs based on our dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "      *epoch_fd_results* : dictionary\n",
    "        dictionary of averaged framewise displacement per 30 second epochs\n",
    "    \"\"\" \n",
    "\n",
    "    epoch_fd_results = []\n",
    "\n",
    "    for subject, file_paths in confounds_files.items():\n",
    "        for file_path in file_paths:\n",
    "            task_run = extract_task_run_from_filename(os.path.basename(file_path))\n",
    "            df = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "            fd_values = df['framewise_displacement'].tolist()\n",
    "\n",
    "            window_size = int(window_size_seconds / 2.1)\n",
    "\n",
    "            num_windows = len(fd_values) // window_size\n",
    "\n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * window_size\n",
    "                end_idx = start_idx + window_size\n",
    "                window_fd_values = fd_values[start_idx:end_idx]\n",
    "                window_fd_values = [fd for fd in window_fd_values if not pd.isna(fd)]\n",
    "\n",
    "                window_data = {\n",
    "                    \"Subject Number\": subject,\n",
    "                    \"File Name\": task_run,\n",
    "                    \"Window Start\": start_idx * 2.1,  \n",
    "                    \"Window End\": end_idx * 2.1,\n",
    "                    \"Average FD\": sum(window_fd_values) / len(window_fd_values),\n",
    "                    \"Exceeding Threshold\": sum(1 for fd in window_fd_values if fd > 0.7)\n",
    "                }\n",
    "\n",
    "                epoch_fd_results.append(window_data)\n",
    "    return epoch_fd_results\n",
    "\n",
    "\n",
    "output_csv_file = \"../output_directory/epoch_fd_report.csv\"\n",
    "confounds_path = get_file_path(\"../output_directory\", \"desc-confounds_timeseries.tsv\")\n",
    "\n",
    "confounds_path=filtered_data = {key: value for key, value in confounds_path.items() if key != 'sub-04'}\n",
    "\n",
    "epoch_fd_results = epoch_fd_avg(confounds_path,window_size_seconds=30)\n",
    "df = pd.DataFrame(epoch_fd_results)\n",
    "df.to_csv(output_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for converting all sleep score tsv files into one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_number(file_name):\n",
    "    # Assuming the subject number is extracted from the file name (modify accordingly)\n",
    "    # This is just a simple example, adjust as per your file naming convention\n",
    "    return file_name[0:6]\n",
    "\n",
    "def create_combined_csv(tsv_folder_path, output_csv_path):\n",
    "    # Initialize an empty DataFrame to store the combined data\n",
    "    df_combined = pd.DataFrame(columns=['Subject Number'])\n",
    "\n",
    "    # Iterate through each TSV file in the folder\n",
    "    for tsv_file in sorted(os.listdir(tsv_folder_path)):\n",
    "        if tsv_file.endswith('.tsv'):\n",
    "            tsv_file_path = os.path.join(tsv_folder_path, tsv_file)\n",
    "\n",
    "            # Read the TSV file\n",
    "            df_tsv = pd.read_csv(tsv_file_path, delimiter='\\t')\n",
    "\n",
    "            # Extract subject number from the file name\n",
    "            subject_number = extract_subject_number(tsv_file)\n",
    "\n",
    "            # Add 'Subject Number' column to the TSV data\n",
    "            df_tsv['Subject Number'] = subject_number\n",
    "\n",
    "            # Select and rename columns\n",
    "            df_tsv = df_tsv[['Subject Number', 'session', 'epoch_start_time_sec', '30-sec_epoch_sleep_stage']]\n",
    "        \n",
    "\n",
    "            # Append the data to the combined DataFrame\n",
    "            df_combined = pd.concat([df_combined, df_tsv],ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    df_combined.to_csv(output_csv_path, sep=',', index=False)\n",
    "\n",
    "# Example usage:\n",
    "tsv_folder_path = '../Dataset/sourcedata'\n",
    "output_csv_path = '../Dataset/sourcedata/all_nrem.csv'\n",
    "create_combined_csv(tsv_folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the Average FD wrt sleep score:\n",
    "        Plot the raincloud plot for visualising FD vs Sleep states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ptitprince as pt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../output_directory/epoch_fd_report.csv\")\n",
    "\n",
    "f,ax=plt.subplots(figsize=(10,5))\n",
    "pt.RainCloud(x=\"30-sec_epoch_sleep_stage\", y=\"Average FD\", data=df, width_viol=0.5, width_box=0.4, orient='h',order= ['W','1','2'])\n",
    "\n",
    "# Set additional properties or customization if needed\n",
    "ax.set_title(\"Framewise Displacement vs Sleep Stages\")\n",
    "ax.set_ylabel(\"Sleep Stages\")\n",
    "ax.set_xlabel(\"Maximum Framewise Displacement(mm)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Subject vs FD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ptitprince as pt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../output_directory/qc_fd_report.csv\")\n",
    "\n",
    "fd_per_subject = df[[\"Subject Number\",\"Average FD\"]].groupby(\"Subject Number\").mean()\n",
    "\n",
    "\n",
    "subject_numbers = fd_per_subject.index.astype(str)  \n",
    "average_fds = fd_per_subject[\"Average FD\"]\n",
    "\n",
    "# Create a list of colors based on the value of FD\n",
    "colors = ['red' if fd > 0.4 else 'green' for fd in average_fds]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(subject_numbers, average_fds, color=colors)\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Subjects\")\n",
    "plt.ylabel(\"Average Framewise Displacement (mm)\")\n",
    "plt.title(\"Average Framewise Displacement by Subject\")\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ptitprince as pt\n",
    "\n",
    "\n",
    "df = pd.read_csv('../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_roi.csv')\n",
    "\n",
    "fd_per_subject = df[[\"sleep_stage\",\"Total datapoints\"]].groupby(\"sleep_stage\").mean()\n",
    "\n",
    "\n",
    "subject_numbers = fd_per_subject.index.astype(str)  \n",
    "average_fds = fd_per_subject[\"Total datapoints\"].drop(\"3\")/28\n",
    "desired_order = [\"W\", \"1\", \"2\"]\n",
    "\n",
    "average_fds = average_fds.reindex(desired_order)\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([\"Wake\", \"NREM1\",\"NREM2\"], average_fds, color='green')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Sleep Scores\")\n",
    "plt.ylabel(\"Data length (min)\")\n",
    "plt.title(\"Average data length per sleep score\") \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing script:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessing import postproc_script \n",
    "\n",
    "img_files = get_file_path(\"../output_directory\", \"desc-preproc_bold.nii.gz\")\n",
    "img__mask_files = get_file_path(\"../output_directory\", \"_desc-brain_mask.nii.gz\")\n",
    "conf_files = get_file_path(\"../output_directory\", \"desc-confounds_timeseries.tsv\")\n",
    "\n",
    "selected_keys = ['sub-31','sub-32','sub-33']\n",
    "\n",
    "# Creating a new dictionary with only the selected keys\n",
    "img_files = {key: img_files[key] for key in selected_keys}\n",
    "img__mask_files = {key: img__mask_files[key] for key in selected_keys}\n",
    "conf_files = {key: conf_files[key] for key in selected_keys}\n",
    "\n",
    "postproc_csv_files = postproc_script(img_files, img__mask_files, conf_files)\n",
    "\n",
    "print(postproc_csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the postprocessed data with the following:\n",
    "    - atlas : Harvard Oxford Atlas: 2mm\n",
    "    - masking : atlas + no confounds\n",
    "    -storing time series of all regions in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ho_region_ts_per_epoch(fmri_path,atlas_dir,window_size_seconds=30,display_report=False):\n",
    "    \"\"\"\n",
    "    The code applies Harvard Oxford Atlas to the fMRI file and extracts time series of each of the 48 regions in windows of 30 seconds.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "      *fmri_path* : string\n",
    "        path to the fmri file\n",
    "\n",
    "      *atlas_dir* : string\n",
    "        path to the Harvard Oxford atlas directory\n",
    "\n",
    "      *window_size_seconds* : int\n",
    "        window size in seconds\n",
    "\n",
    "      *display_report* : boolean\n",
    "        parameter to decide to display the report of the atlas or not\n",
    "      \n",
    "    Returns:\n",
    "    --------\n",
    "      *region_ts* : 2D array\n",
    "        a 2D array having \n",
    "          columns: representing each of the 48 region in the HO atlas \n",
    "          rows: representing the time series of window_size_seconds\n",
    "    \"\"\" \n",
    "\n",
    "    # Fetch cortical atlas\n",
    "    cortical_atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm', data_dir=atlas_dir)\n",
    "    cortical_maps = cortical_atlas.maps\n",
    "    cortical_labels = cortical_atlas.labels\n",
    "    cortical_masker = maskers.NiftiLabelsMasker(labels_img=cortical_maps, labels= cortical_labels)\n",
    "\n",
    "\n",
    "    # Fetch subcortical atlas\n",
    "    subcortical_atlas = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm', data_dir=atlas_dir)\n",
    "    subcortical_maps = subcortical_atlas.maps\n",
    "    subcortical_labels = subcortical_atlas.labels\n",
    "    subcortical_masker = maskers.NiftiLabelsMasker(labels_img=subcortical_maps, labels= subcortical_labels)\n",
    "    \n",
    "    \n",
    "    combined_labels = cortical_labels[1:] + subcortical_labels[1:]  # Skip 'Background' label from subcortical labels\n",
    "    n_regions = len(combined_labels)\n",
    "\n",
    "    # Apply masker\n",
    "    time_series_cortical = cortical_masker.fit_transform(fmri_path, confounds=None)\n",
    "    time_series_subcortical = subcortical_masker.fit_transform(fmri_path, confounds=None)\n",
    "    time_series_combined = np.concatenate((time_series_cortical,time_series_subcortical),axis=1)\n",
    "\n",
    "    # Display report if needed\n",
    "    if display_report:\n",
    "        print(\"Dimension of the fMRI signal (x,y,z,timestamps): \", nibabel.load(fmri_path).shape)\n",
    "        print(\"Dimension of the combined time series (timestamps, voxels): \", time_series_combined.shape)\n",
    "        print(f'Combined atlas has {n_regions} ROIs.')\n",
    "        print(combined_labels)\n",
    "\n",
    "    window_size = int(window_size_seconds / 2.1)  # The normal window size\n",
    "    first_window_correction = 5  # TRs to remove for the first epoch\n",
    "\n",
    "    # Adjusted to account for the shortened first window\n",
    "    adjusted_length = len(time_series_combined[:, 0]) - first_window_correction\n",
    "    num_windows = 2 + ((adjusted_length - window_size) // window_size)\n",
    "    n_regions = time_series_combined.shape[1]\n",
    "    region_ts = np.empty((num_windows, n_regions), dtype=object)\n",
    "    region_index = 0\n",
    "\n",
    "    for region in zip(*time_series_combined):\n",
    "        # Special handling for the first window\n",
    "        start_idx = 0\n",
    "        end_idx = window_size - first_window_correction\n",
    "        region_ts[0, region_index] = region[start_idx:end_idx]\n",
    "\n",
    "        # Process subsequent windows\n",
    "        for i in range(1, num_windows):\n",
    "            start_idx = (i * window_size) - first_window_correction\n",
    "            end_idx = start_idx + window_size\n",
    "            region_ts[i, region_index] = region[start_idx:end_idx]\n",
    "\n",
    "        region_index += 1\n",
    "\n",
    "    # Construct DataFrame with appropriate column names\n",
    "    columns = [f'{label}' for i, label in enumerate(combined_labels, start=1)]\n",
    "    region_ts_df = pd.DataFrame(region_ts, columns=columns)\n",
    "    return region_ts_df\n",
    "\n",
    "\n",
    "atlas_dir = \"../python_scripts/resources/atlas\"\n",
    "fmri_files = get_file_path(\"../output_directory\", \"postproc_smooth.nii.gz\")\n",
    "# selected_keys = ['sub-02']\n",
    "\n",
    "# # # Creating a new dictionary with only the selected keys\n",
    "# fmri_files = {key: fmri_files[key] for key in selected_keys}\n",
    "\n",
    "ho_ts_dfs = []\n",
    "for subject, fmri_file in fmri_files.items():\n",
    "        for file_path in fmri_file:\n",
    "          region_ts_df = get_ho_region_ts_per_epoch(file_path, atlas_dir,window_size_seconds=30,display_report=False)\n",
    "          region_ts_df.insert(0, 'run', extract_task_run_from_filename(os.path.basename(file_path)))\n",
    "          region_ts_df.insert(0, 'subject', subject)\n",
    "          ho_ts_dfs.append(region_ts_df)\n",
    "          print(f\"{extract_task_run_from_filename(os.path.basename(file_path))} of {subject} successfully completed\")\n",
    "\n",
    "result_df = pd.concat(ho_ts_dfs, ignore_index=True)\n",
    "\n",
    "result_df.to_csv('../output_directory/evaluation_dataset/harvard_atlas_dataset/ho_atlas_dataset_corrected_filtered.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script helping to get a csv format of all the data in a structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Reading the uploaded CSV file\n",
    "df = read_csv('../output_directory/evaluation_dataset/harvard_atlas_dataset/ho_atlas_dataset_corrected_filtered.csv')\n",
    "\n",
    "def process_data(df):\n",
    "    # Renaming columns for clarity\n",
    "    df.columns.values[0:3] = ['subject', 'run', 'sleep_stage']\n",
    "\n",
    "    # Identify consecutive series and assign group IDs\n",
    "    consecutive_series = (df['subject'] != df['subject'].shift(1)) | \\\n",
    "                         (df['run'] != df['run'].shift(1)) | \\\n",
    "                         (df['sleep_stage'] != df['sleep_stage'].shift(1))\n",
    "    group_id = consecutive_series.cumsum()\n",
    "    \n",
    "    # Group by 'subject', 'run', 'sleep_stage', and group_id\n",
    "    grouped = df.groupby([df.columns[0], df.columns[1], df.columns[2], group_id],sort=False)\n",
    "    \n",
    "    aggregation = {col: lambda x: list(x) for col in df.columns[3:]}\n",
    "    processed_data = grouped.agg(aggregation).reset_index()\n",
    "\n",
    "    processed_data.insert(4, 'Total datapoints', processed_data.iloc[:,4].apply(lambda x: len(x)))\n",
    "    # processed_data['Total datapoints'] = processed_data.apply(lambda row: 9 + (len(row.iloc[4]) - 1) * 14, axis=1)\n",
    "    processed_data = processed_data.drop('level_3', axis=1)\n",
    "    return processed_data\n",
    "\n",
    "processed_df = process_data(df)\n",
    "\n",
    "def write_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "output_file_path = '../output_directory/evaluation_dataset/harvard_atlas_dataset/eval_dataset_corrected_filtered.csv'\n",
    "write_to_csv(processed_df, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurst Value Evaluation of the final csv dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Convert string representation of list of tuples to a list of floats\n",
    "def convert_string_to_list(data_string):\n",
    "    data_string = data_string[1:-1]\n",
    "    tuples = re.findall(r'\\((.*?)\\)', data_string)\n",
    "    list_of_floats = [float(num) for t in tuples for num in t.split(', ')]\n",
    "    return list_of_floats\n",
    "\n",
    "# Reading the uploaded CSV file\n",
    "df = read_csv('../output_directory/evaluation_dataset/harvard_atlas_dataset/eval_dataset_corrected_filtered.csv')\n",
    "\n",
    "\n",
    "fit_scores={}\n",
    "region_columns = df.columns[4:]\n",
    "\n",
    "\n",
    "for region in region_columns:\n",
    "    fit_scores[region] = df.apply(lambda row: calculate_hurst_dfa(convert_string_to_list(row[region]))[1], axis=1).values\n",
    "    df[region] = df.apply(lambda row: calculate_hurst_dfa(convert_string_to_list(row[region]))[0], axis=1)\n",
    "    print(f\"{region} completed\")\n",
    "\n",
    "\n",
    "#Fit score calculation:\n",
    "# fit_scores= {key: [item for sublist in value for item in sublist] for key, value in fit_scores.items()}\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected_filtered.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def safe_eval(val):\n",
    "    try:\n",
    "        # Evaluate the string value as a list\n",
    "        evaluated = ast.literal_eval(val)\n",
    "        # Check if the evaluated list contains only NaN\n",
    "        if all(np.isnan(item) if isinstance(item, float) else False for item in evaluated):\n",
    "            return [0]  # Return list with a single NaN\n",
    "        return evaluated\n",
    "    except:\n",
    "        # Return a list with a single NaN if the value can't be evaluated as a list\n",
    "        return [0]\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected_filtered.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify Hurst columns\n",
    "hurst_columns = df.columns[4:]\n",
    "\n",
    "# Expand each row into multiple rows\n",
    "expanded_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    # Convert the string representations to lists (handling NaN values)\n",
    "    list_values = [safe_eval(row[col]) for col in hurst_columns]\n",
    "\n",
    "    # Find the length of the arrays (assuming all arrays are of the same length)\n",
    "    length = max(len(l) for l in list_values)\n",
    "\n",
    "    # Create a new row for each element in the arrays\n",
    "    for i in range(length):\n",
    "        new_row = row.to_dict()\n",
    "        for idx, col in enumerate(hurst_columns):\n",
    "            new_row[col] = list_values[idx][i] if i < len(list_values[idx]) else 0\n",
    "        expanded_rows.append(new_row)\n",
    "    \n",
    "\n",
    "# Create a new DataFrame with the expanded rows\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "expanded_df = expanded_df[expanded_df['Total datapoints'] >= 8]\n",
    "state_list= expanded_df[\"run\"].apply(lambda x: 'rest' if 'rest' in x else 'sleep')\n",
    "expanded_df.insert(2, 'state', state_list)\n",
    "expanded_df['sleep_stage'] = expanded_df['sleep_stage'].apply(lambda x: \"W\" if (x.split()[0] == 'W') else x.split()[0])\n",
    "\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "expanded_df.to_csv('../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected_filtered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope Fit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_slope_fit = {}\n",
    "for key, values in fit_scores.items():\n",
    "    non_zero_values = [value for value in values if value != 0]\n",
    "    mean_slope_fit[key] = np.mean(non_zero_values)\n",
    "\n",
    "keys = list(mean_slope_fit.keys())\n",
    "values = list(mean_slope_fit.values())\n",
    "threshold_value_y =0.95\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(keys, values, color='skyblue')\n",
    "plt.axhline(y=threshold_value_y, color='r', linestyle='--', label=f'Threshold Fit score: {threshold_value_y}')\n",
    "plt.xlabel('Regions')\n",
    "plt.ylabel('Mean of fit score')\n",
    "plt.title('Mean of fit score for Each Region')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Convert string representation of list of tuples to a list of floats\n",
    "def convert_string_to_list(data_string):\n",
    "    data_string = data_string[1:-1]\n",
    "    tuples = re.findall(r'\\((.*?)\\)', data_string)\n",
    "    list_of_floats = [float(num) for t in tuples for num in t.split(', ')]\n",
    "    return list_of_floats\n",
    "\n",
    "\n",
    "def calculate_variance(data):\n",
    "    # Calculate the mean of the data\n",
    "    mean = sum(data) / len(data)\n",
    "    \n",
    "    # Calculate the squared differences from the mean\n",
    "    squared_diff = [(x - mean) ** 2 for x in data]\n",
    "    \n",
    "    # Calculate the variance as the mean of the squared differences\n",
    "    variance = sum(squared_diff) / len(data)\n",
    "    \n",
    "    return variance\n",
    "\n",
    "# Reading the uploaded CSV file\n",
    "df = read_csv('../output_directory/evaluation_dataset/eval_dataset.csv')\n",
    "\n",
    "\n",
    "fit_scores={}\n",
    "region_columns = df.columns[4:]\n",
    "\n",
    "\n",
    "for region in region_columns:\n",
    "    df[region] = df.apply(lambda row: calculate_variance(convert_string_to_list(row[region])), axis=1)\n",
    "    print(f\"{region} completed\")\n",
    "\n",
    "df = df[df['Total datapoints'] >= 112]\n",
    "state_list= df[\"run\"].apply(lambda x: 'rest' if 'rest' in x else 'sleep')\n",
    "df.insert(2, 'state', state_list)\n",
    "df['sleep_stage'] = df['sleep_stage'].apply(lambda x: \"W\" if (x.split()[0] == 'W') else x.split()[0])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('../output_directory/evaluation_dataset/variance_dataset.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising Scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import datasets, plotting\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def calculate_mean(file_path, parameter_index):\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    columns_to_consider = [df.columns[parameter_index]] + list(df.columns[5:])\n",
    "\n",
    "    # Filter out zero values\n",
    "    non_zero_df = df[df[columns_to_consider] != 0]\n",
    "\n",
    "    # Group by parameter and calculate the mean for each region\n",
    "    mean_non_zero_hurst_exponents = non_zero_df.groupby(df.columns[parameter_index], as_index=False).mean()\n",
    "\n",
    "    return mean_non_zero_hurst_exponents\n",
    "\n",
    "def create_hurst_nifti_cortical(mean_hurst_values):\n",
    "    # Fetch cortical atlas\n",
    "    atlas_dir = \"../python_scripts/resources/atlas\"\n",
    "    cortical_atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm', data_dir=atlas_dir)\n",
    "    cortical_labels = cortical_atlas.labels\n",
    "\n",
    "    # Load the atlas image and get the data\n",
    "    atlas_img = nib.load(cortical_atlas.filename)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "    hurst_data = np.zeros_like(atlas_data)\n",
    "\n",
    "\n",
    "    for i, region in enumerate(cortical_labels[1:]):  # Skip the background label\n",
    "        # Set all voxels with the label i+1 to the mean Hurst value\n",
    "        hurst_data[atlas_data == i+1] = mean_hurst_values[region]\n",
    "    hurst_img_cortical = nib.Nifti1Image(hurst_data, atlas_img.affine)\n",
    "    return hurst_img_cortical\n",
    "\n",
    "\n",
    "def create_hurst_nifti_subcortical(mean_hurst_values):\n",
    "    # Fetch subcortical atlas\n",
    "    atlas_dir = \"../python_scripts/resources/atlas\"\n",
    "    subcortical_atlas = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm', data_dir=atlas_dir)\n",
    "    subcortical_labels = subcortical_atlas.labels\n",
    "\n",
    "    # Load the atlas image and get the data\n",
    "    atlas_img = nib.load(subcortical_atlas.filename)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "    hurst_data = np.zeros_like(atlas_data)\n",
    "\n",
    "\n",
    "    for i, region in enumerate(subcortical_labels[1:]):  # Skip the background label\n",
    "        # Set all voxels with the label i+1 to the mean Hurst value\n",
    "        hurst_data[atlas_data == i+1] = mean_hurst_values[region]\n",
    "    hurst_img_subcortical = nib.Nifti1Image(hurst_data, atlas_img.affine)\n",
    "    \n",
    "    return hurst_img_subcortical\n",
    "\n",
    "\n",
    "def plot_brain_regions_hurst(group_parameter, hurst_img_cortical, hurst_img_subcortical):\n",
    "    fig, axes = plt.subplots(nrows=len(hurst_img_cortical), ncols=1, figsize=(10, 10))\n",
    "    fig.text(0.5, 1, f'Hurst Exponent', va='center', ha='center', rotation='horizontal', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Define the color map\n",
    "    cmap = plotting.cm.bwr_r\n",
    "\n",
    "    # Plot for cortical region\n",
    "    for i, (stage, image) in enumerate(hurst_img_cortical.items()):\n",
    "        data_cortical = image.get_fdata()\n",
    "\n",
    "        data_subcortical = hurst_img_subcortical[stage].get_fdata()\n",
    "\n",
    "        display = plotting.plot_stat_map(\n",
    "            image,\n",
    "            title=group_parameter[i],\n",
    "            display_mode='ortho',\n",
    "            colorbar=False,\n",
    "            axes=axes[i],\n",
    "            annotate=False, draw_cross=False,\n",
    "            figure=fig,\n",
    "            symmetric_cbar=False,\n",
    "            cmap=cmap,\n",
    "        )\n",
    "        display.add_overlay(hurst_img_subcortical[stage], cmap=cmap,vmax=1, vmin=0.70, colorbar=True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for visualising hurst exponent vs Sleep score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected.csv'\n",
    "mean_hurst_per_sleep_stage = calculate_mean(file_path, 3)\n",
    "sleep_stage_dict_cortical = {}\n",
    "sleep_stage_dict_subcortical = {}\n",
    "\n",
    "# Iterate over each sleep stage and create a plot\n",
    "for _, row in mean_hurst_per_sleep_stage.iterrows():\n",
    "    sleep_stage = row['sleep_stage']\n",
    "    sleep_stage_dict_cortical[sleep_stage] = create_hurst_nifti_cortical(row)\n",
    "    sleep_stage_dict_subcortical[sleep_stage] = create_hurst_nifti_subcortical(row)\n",
    "\n",
    "sleep_stage_dict_cortical = {'W': sleep_stage_dict_cortical['W'], '1': sleep_stage_dict_cortical['1'], '2': sleep_stage_dict_cortical['2']}\n",
    "sleep_stage_dict_subcortical = {'W': sleep_stage_dict_subcortical['W'], '1': sleep_stage_dict_subcortical['1'], '2': sleep_stage_dict_subcortical['2']}\n",
    "\n",
    "plot_brain_regions_hurst([\"Wake\",\"NREM 1\",\"NREM 2\"], sleep_stage_dict_cortical,sleep_stage_dict_subcortical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../output_directory/evaluation_dataset/dfa_hurst_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a new column to group sleep stages as 'W' and '1/2'\n",
    "df['sleep_stage'] = df['sleep_stage'].replace({'W': 'W', '1': '1/2', '2': '1/2','3':'1/2'})\n",
    "\n",
    "# Define the columns to consider for grouping and calculating mean\n",
    "columns_to_consider = [df.columns[3]] + list(df.columns[5:])\n",
    "\n",
    "\n",
    "# Filter out zero values\n",
    "non_zero_df = df[df[columns_to_consider] != 0]\n",
    "\n",
    "# Group by 'sleep_stage_grouped' and calculate the mean for each group\n",
    "mean_hurst_per_state = non_zero_df.groupby([df.columns[3]], as_index=False).mean()\n",
    "\n",
    "# print(mean_hurst_per_state)\n",
    "\n",
    "state_dict_cortical = {}\n",
    "state_dict_subcortical = {}\n",
    "\n",
    "# Iterate over each sleep stage and create a plot\n",
    "for _, row in mean_hurst_per_state.iterrows():\n",
    "    state = row['sleep_stage']\n",
    "    state_dict_cortical[state] = create_hurst_nifti_cortical(row)\n",
    "    state_dict_subcortical[state] = create_hurst_nifti_subcortical(row)\n",
    "\n",
    "state_dict_cortical = {'W': state_dict_cortical['W'], '1/2': state_dict_cortical['1/2']}\n",
    "state_dict_subcortical = {'W': state_dict_subcortical['W'], '1/2': state_dict_subcortical['1/2']}\n",
    "\n",
    "plot_brain_regions_hurst([\"Rest\",\"Sleep\"], state_dict_cortical,state_dict_subcortical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = '../output_directory/evaluation_dataset/dfa_hurst_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_consider = [df.columns[1],df.columns[3]] + list(df.columns[5:])\n",
    "\n",
    "# Group by parameter and calculate the mean for each region\n",
    "mean_hurst_per_time = df[columns_to_consider].groupby([df.columns[1],df.columns[3]],as_index=False).mean()\n",
    "\n",
    "time_dict_cortical = {}\n",
    "time_dict_subcortical = {}\n",
    "\n",
    "# Iterate over each sleep stage and create a plot\n",
    "for _, row in mean_hurst_per_time.iterrows():\n",
    "    run = row['run'] + \"_\" + row['sleep_stage'] \n",
    "    time_dict_cortical[run] = create_hurst_nifti_cortical(row)\n",
    "    time_dict_subcortical[run] = create_hurst_nifti_subcortical(row)\n",
    "\n",
    "\n",
    "time_dict_cortical_t1 = {'W': time_dict_cortical['task-rest_run-1_W'], '1': time_dict_cortical['task-rest_run-1_1'], '2': time_dict_cortical['task-rest_run-1_2']}\n",
    "time_dict_subcortical_t1 = {'W': time_dict_subcortical['task-rest_run-1_W'], '1': time_dict_subcortical['task-rest_run-1_1'], '2': time_dict_subcortical['task-rest_run-1_2']}\n",
    "\n",
    "time_dict_cortical_t2 = {'W': time_dict_cortical['task-rest_run-2_W'], '1': time_dict_cortical['task-rest_run-2_1'], '2': time_dict_cortical['task-rest_run-2_2']}\n",
    "time_dict_subcortical_t2 = {'W': time_dict_subcortical['task-rest_run-2_W'], '1': time_dict_subcortical['task-rest_run-2_1'], '2': time_dict_subcortical['task-rest_run-2_2']}\n",
    "\n",
    "time_dict_cortical_t3 = {'W': time_dict_cortical['task-sleep_run-1_W'], '1': time_dict_cortical['task-sleep_run-1_1'], '2': time_dict_cortical['task-sleep_run-1_2']}\n",
    "time_dict_subcortical_t3 = {'W': time_dict_subcortical['task-sleep_run-1_W'], '1': time_dict_subcortical['task-sleep_run-1_1'], '2': time_dict_subcortical['task-sleep_run-1_2']}\n",
    "\n",
    "time_dict_cortical_t4 = {'W': time_dict_cortical['task-sleep_run-2_W'], '1': time_dict_cortical['task-sleep_run-2_1'], '2': time_dict_cortical['task-sleep_run-2_2']}\n",
    "time_dict_subcortical_t4 = {'W': time_dict_subcortical['task-sleep_run-2_W'], '1': time_dict_subcortical['task-sleep_run-2_1'], '2': time_dict_subcortical['task-sleep_run-2_2']}\n",
    "\n",
    "time_dict_cortical_t5 = {'W': time_dict_cortical['task-sleep_run-3_W'], '1': time_dict_cortical['task-sleep_run-3_1'], '2': time_dict_cortical['task-sleep_run-3_2']}\n",
    "time_dict_subcortical_t5 = {'W': time_dict_subcortical['task-sleep_run-3_W'], '1': time_dict_subcortical['task-sleep_run-3_1'], '2': time_dict_subcortical['task-sleep_run-3_2']}\n",
    "\n",
    "time_dict_cortical_t6 = {'W': time_dict_cortical['task-sleep_run-4_W'], '1': time_dict_cortical['task-sleep_run-4_1'], '2': time_dict_cortical['task-sleep_run-4_2']}\n",
    "time_dict_subcortical_t6 = {'W': time_dict_subcortical['task-sleep_run-4_W'], '1': time_dict_subcortical['task-sleep_run-4_1'], '2': time_dict_subcortical['task-sleep_run-4_2']}\n",
    "\n",
    "time_dict_cortical_t7 = {'W': time_dict_cortical['task-sleep_run-5_W'], '1': time_dict_cortical['task-sleep_run-5_1'], '2': time_dict_cortical['task-sleep_run-5_2']}\n",
    "time_dict_subcortical_t7 = {'W': time_dict_subcortical['task-sleep_run-5_W'], '1': time_dict_subcortical['task-sleep_run-5_1'], '2': time_dict_subcortical['task-sleep_run-5_2']}\n",
    "\n",
    "time_dict_cortical_t8 = {'W': time_dict_cortical['task-sleep_run-6_W'], '1': time_dict_cortical['task-sleep_run-6_1'], '2': time_dict_cortical['task-sleep_run-6_2']}\n",
    "time_dict_subcortical_t8 = {'W': time_dict_subcortical['task-sleep_run-6_W'], '1': time_dict_subcortical['task-sleep_run-6_1'], '2': time_dict_subcortical['task-sleep_run-6_2']}\n",
    "\n",
    "time_dict_cortical_t9 = {'W': time_dict_cortical['task-sleep_run-7_W'], '1': time_dict_cortical['task-sleep_run-7_1'], '2': time_dict_cortical['task-sleep_run-7_2']}\n",
    "time_dict_subcortical_t9 = {'W': time_dict_subcortical['task-sleep_run-7_W'], '1': time_dict_subcortical['task-sleep_run-7_1'], '2': time_dict_subcortical['task-sleep_run-7_2']}\n",
    "\n",
    "\n",
    "time_dict_cortical_wake = {'W_1': time_dict_cortical['task-rest_run-1_W'], 'W_2': time_dict_cortical['task-rest_run-2_W'], 'W_3': time_dict_cortical['task-sleep_run-1_W'],'W_4': time_dict_cortical['task-sleep_run-2_W'],'W_5': time_dict_cortical['task-sleep_run-3_W'],'W_6': time_dict_cortical['task-sleep_run-4_W'],'W_7': time_dict_cortical['task-sleep_run-5_W'],'W_8': time_dict_cortical['task-sleep_run-6_W'],'W_9': time_dict_cortical['task-sleep_run-7_W']}\n",
    "time_dict_cortical_1 = {'1_1': time_dict_cortical['task-rest_run-1_1'], '1_2': time_dict_cortical['task-rest_run-2_1'], '1_3': time_dict_cortical['task-sleep_run-1_1'],'1_4': time_dict_cortical['task-sleep_run-2_1'],'1_5': time_dict_cortical['task-sleep_run-3_1'],'1_6': time_dict_cortical['task-sleep_run-4_1'],'1_7': time_dict_cortical['task-sleep_run-5_1'],'1_8': time_dict_cortical['task-sleep_run-6_1'],'1_9': time_dict_cortical['task-sleep_run-7_1']}\n",
    "time_dict_cortical_2 = {'2_1': time_dict_cortical['task-rest_run-1_2'], '2_2': time_dict_cortical['task-rest_run-2_2'], '2_3': time_dict_cortical['task-sleep_run-1_2'],'2_4': time_dict_cortical['task-sleep_run-2_2'],'2_5': time_dict_cortical['task-sleep_run-3_2'],'2_6': time_dict_cortical['task-sleep_run-4_2'],'2_7': time_dict_cortical['task-sleep_run-5_2'],'2_8': time_dict_cortical['task-sleep_run-6_2'],'2_9': time_dict_cortical['task-sleep_run-7_2']}\n",
    "\n",
    "time_dict_subcortical_wake = {'W_1': time_dict_subcortical['task-rest_run-1_W'], 'W_2': time_dict_subcortical['task-rest_run-2_W'], 'W_3': time_dict_subcortical['task-sleep_run-1_W'],'W_4': time_dict_subcortical['task-sleep_run-2_W'],'W_5': time_dict_subcortical['task-sleep_run-3_W'],'W_6': time_dict_subcortical['task-sleep_run-4_W'],'W_7': time_dict_subcortical['task-sleep_run-5_W'],'W_8': time_dict_subcortical['task-sleep_run-6_W'],'W_9': time_dict_subcortical['task-sleep_run-7_W']}\n",
    "time_dict_subcortical_1 = {'1_1': time_dict_subcortical['task-rest_run-1_1'], '1_2': time_dict_subcortical['task-rest_run-2_1'], '1_3': time_dict_subcortical['task-sleep_run-1_1'],'1_4': time_dict_subcortical['task-sleep_run-2_1'],'1_5': time_dict_subcortical['task-sleep_run-3_1'],'1_6': time_dict_subcortical['task-sleep_run-4_1'],'1_7': time_dict_subcortical['task-sleep_run-5_1'],'1_8': time_dict_subcortical['task-sleep_run-6_1'],'1_9': time_dict_subcortical['task-sleep_run-7_1']}\n",
    "time_dict_subcortical_2 = {'2_1': time_dict_subcortical['task-rest_run-1_2'], '2_2': time_dict_subcortical['task-rest_run-2_2'], '2_3': time_dict_subcortical['task-sleep_run-1_2'],'2_4': time_dict_subcortical['task-sleep_run-2_2'],'2_5': time_dict_subcortical['task-sleep_run-3_2'],'2_6': time_dict_subcortical['task-sleep_run-4_2'],'2_7': time_dict_subcortical['task-sleep_run-5_2'],'2_8': time_dict_subcortical['task-sleep_run-6_2'],'2_9': time_dict_subcortical['task-sleep_run-7_2']}\n",
    "\n",
    "\n",
    "parameters = [([\"T1 (W)\", \"T2 (W)\", \"T3 (W)\",\"T4 (W)\",\"T5 (W)\",\"T6 (W)\",\"T7 (W)\",\"T8 (W)\",\"T9 (W)\"], time_dict_cortical_wake, time_dict_subcortical_wake),\n",
    "              ([\"T1 (N1)\", \"T2 (N1)\", \"T3 (N1)\",\"T4 (N1)\",\"T5 (N1)\",\"T6 (N1)\",\"T7 (N1)\",\"T8 (N1)\",\"T9 (N1)\"], time_dict_cortical_1, time_dict_subcortical_1),\n",
    "              ([\"T1 (N2)\", \"T2 (N2)\", \"T3 (N2)\",\"T4 (N2)\",\"T5 (N2)\",\"T6 (N2)\",\"T7 (N2)\",\"T8 (N2)\",\"T9 (N2)\"], time_dict_cortical_2, time_dict_subcortical_2)]\n",
    "# [\n",
    "#     ([\"T1 (W)\", \"T1 (N1)\", \"T1 (N2)\"], time_dict_cortical_t1, time_dict_subcortical_t1),\n",
    "#     ([\"T2 (W)\", \"T2 (N1)\", \"T2 (N2)\"], time_dict_cortical_t2, time_dict_subcortical_t2),\n",
    "#     ([\"T3 (W)\", \"T3 (N1)\", \"T3 (N2)\"], time_dict_cortical_t3, time_dict_subcortical_t3),\n",
    "#     ([\"T4 (W)\", \"T4 (N1)\", \"T4 (N2)\"], time_dict_cortical_t4, time_dict_subcortical_t4),\n",
    "#     ([\"T5 (W)\", \"T5 (N1)\", \"T5 (N2)\"], time_dict_cortical_t5, time_dict_subcortical_t5),\n",
    "#     ([\"T6 (W)\", \"T6 (N1)\", \"T6 (N2)\"], time_dict_cortical_t6, time_dict_subcortical_t6),\n",
    "#     ([\"T7 (W)\", \"T7 (N1)\", \"T7 (N2)\"], time_dict_cortical_t7, time_dict_subcortical_t7),\n",
    "#     ([\"T8 (W)\", \"T8 (N1)\", \"T8 (N2)\"], time_dict_cortical_t8, time_dict_subcortical_t8),\n",
    "#     ([\"T9 (W)\", \"T9 (N1)\", \"T9 (N2)\"], time_dict_cortical_t9, time_dict_subcortical_t9)\n",
    "# ]\n",
    "\n",
    "# Plot each combination\n",
    "for i, (group_parameter, hurst_img_cortical, hurst_img_subcortical) in enumerate(parameters, start=1):\n",
    "    plot_brain_regions_hurst(group_parameter, hurst_img_cortical, hurst_img_subcortical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the transition of HE with sleep scores within a subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subject_he(file_name,subject_name,region_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    sub_1_data = df[df['subject'] == subject_name]\n",
    "\n",
    "    he_values = sub_1_data[region_name].tolist()\n",
    "    runs = sub_1_data[\"run\"].tolist()\n",
    "    total_datapoints = sub_1_data[\"Total datapoints\"].tolist()\n",
    "    sleep_scores = sub_1_data[\"sleep_stage\"].tolist()\n",
    "\n",
    "    cumulative_sum = [sum(total_datapoints[:i+1]) for i in range(len(total_datapoints))]\n",
    "    time_line = (np.array(cumulative_sum))*0.5\n",
    "\n",
    "    # Use the indices of sleep_scores as the x-axis values\n",
    "    x_values = list(range(len(time_line)))\n",
    "    positive_he = np.array(he_values) > 0\n",
    "    x_values = np.array(x_values)\n",
    "    he_values = np.array(he_values)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(x_values, he_values, marker='o')  # 'o' for circle markers\n",
    "    plt.plot(x_values[positive_he], he_values[positive_he],\"--\")\n",
    "\n",
    "    # Loop through each point to add a text label\n",
    "    for i, (x, y) in enumerate(zip(x_values, he_values)):\n",
    "        plt.text(x, y, sleep_scores[i][0], fontsize=15, ha='right', va='bottom')\n",
    "        plt.text(x, y, total_datapoints[i]*0.5, fontsize=9, ha='left', va='top')\n",
    "\n",
    "    # Set the x-axis ticks to correspond to the indices of sleep_scores\n",
    "    plt.xticks(x_values, time_line,rotation=90)\n",
    "    plt.grid()\n",
    "    # Adding labels and title for clarity\n",
    "    plt.xlabel('Time (mins)')\n",
    "    plt.ylabel('HE Values')\n",
    "    plt.title(f'HE Values by Sleep Stage of {subject_name}')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "file_name = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected_all_datapoints.csv'\n",
    "subject_name = [\"sub-01\",\"sub-03\",\"sub-05\",\"sub-07\",\"sub-08\",\"sub-09\",\"sub-11\",\"sub-13\",\"sub-15\",\"sub-17\",\"sub-19\",\"sub-22\"]\n",
    "region_name = \"Left Thalamus\"\n",
    "for i in range(len(subject_name)):\n",
    "    plot_subject_he(file_name,subject_name[i],region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots per total datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_box_plot(W_data,_1_data ,_2_data,datapoints,samples_w,samples_1,samples_2):\n",
    "\n",
    "    # Combine NREM1 and NREM2 data for one of the boxplots\n",
    "    sleep_data = _1_data + _2_data\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Plot boxplots for Wake vs Sleep\n",
    "    plt.subplot(2, 1, 1)\n",
    "    boxplot = plt.boxplot([W_data, sleep_data], labels=['Wake', 'Sleep'], \n",
    "                        showmeans=True, showfliers=False, meanline=True, sym='.')\n",
    "\n",
    "    # Customize mean lines\n",
    "    for mean in boxplot['means']:\n",
    "        mean.set_color('green')\n",
    "        mean.set_linewidth(2)\n",
    "        mean.set_linestyle('--')\n",
    "        x, y = mean.get_xdata(), mean.get_ydata()\n",
    "        mean_value = y[0]\n",
    "        plt.text(x[0], mean_value, f'mean: {mean_value:.2f}', \n",
    "                va='bottom', ha='left', color='green')  # Annotate mean value\n",
    "\n",
    "    plt.xlabel('Subject State')\n",
    "    plt.ylabel('Average Brain Hurst Exponent')\n",
    "    plt.title(f'Box Plot of Average Hurst exponent by Wake vs Sleep for {(datapoints)*0.5} mins')\n",
    "    plt.legend([f'Wake: {samples_w} samples , Sleep: {samples_1+samples_2} samples'])\n",
    "\n",
    "    # Plot boxplots for Wake vs NREM1 vs NREM2\n",
    "    plt.subplot(2, 1, 2)\n",
    "    boxplot = plt.boxplot([W_data, _1_data, _2_data], labels=['Wake', 'NREM1', 'NREM2'], \n",
    "                        showmeans=True, showfliers=False, meanline=True, sym='.')\n",
    "\n",
    "    # Customize mean lines\n",
    "    for mean in boxplot['means']:\n",
    "        mean.set_color('green')\n",
    "        mean.set_linewidth(2)\n",
    "        mean.set_linestyle('--')\n",
    "        x, y = mean.get_xdata(), mean.get_ydata()\n",
    "        mean_value = y[0]\n",
    "        plt.text(x[0], mean_value, f'mean: {mean_value:.2f}', \n",
    "                va='bottom', ha='left', color='green')  # Annotate mean value\n",
    "\n",
    "    plt.xlabel('Sleep Score')\n",
    "    plt.ylabel('Average Hurst Exponent')\n",
    "    plt.title(f'Box Plot of Average Hurst exponent by Wake vs NREM1 vs NREM2 for {(datapoints)*0.5} mins')\n",
    "    plt.legend([f'Wake: {samples_w} samples , NREM1: {samples_1} samples, NREM2: {samples_2} samples'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the data\n",
    "file_path = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Get all unique values of 'Total datapoints' column\n",
    "total_datapoints_values = df['Total datapoints'].unique()\n",
    "\n",
    "columns_to_consider = [df.columns[3]] + [df.columns[4]] + list(df.columns[5:])\n",
    "\n",
    "# Group the DataFrame by 'sleep_stage' and 'Total datapoints'\n",
    "grouped_data_w = df[df['sleep_stage'] == 'W'].groupby(['Total datapoints'])\n",
    "grouped_data_1 = df[df['sleep_stage'] == '1'].groupby(['Total datapoints'])\n",
    "grouped_data_2 = df[df['sleep_stage'] == '2'].groupby(['Total datapoints'])\n",
    "\n",
    "run_freq_w = grouped_data_w['Total datapoints'].value_counts().to_dict()\n",
    "run_freq_1 = grouped_data_1['Total datapoints'].value_counts().to_dict()\n",
    "run_freq_2 = grouped_data_2['Total datapoints'].value_counts().to_dict()\n",
    "\n",
    "grouped_data_w = df[df['sleep_stage'] == 'W'][columns_to_consider].groupby(['sleep_stage', 'Total datapoints']).mean()\n",
    "grouped_data_1 = df[df['sleep_stage'] == '1'][columns_to_consider].groupby(['sleep_stage', 'Total datapoints']).mean()\n",
    "grouped_data_2 = df[df['sleep_stage'] == '2'][columns_to_consider].groupby(['sleep_stage', 'Total datapoints']).mean()\n",
    "\n",
    "\n",
    "# Filter the DataFrame for sleep stage 'W'\n",
    "W_data = grouped_data_w.reset_index()\n",
    "W_total_datapoints = W_data['Total datapoints'].unique()\n",
    "\n",
    "# Filter the DataFrame for sleep stage '1'\n",
    "_1_data = grouped_data_1.reset_index()\n",
    "_1_total_datapoints = _1_data['Total datapoints'].unique()\n",
    "\n",
    "# Filter the DataFrame for sleep stage '2'\n",
    "_2_data = grouped_data_2.reset_index()\n",
    "_2_total_datapoints = _2_data['Total datapoints'].unique()\n",
    "\n",
    "# Compare the 'Total datapoints' values\n",
    "common_total_datapoints = set(W_total_datapoints) & set(_1_total_datapoints) & set(_2_total_datapoints)\n",
    "\n",
    "all_figures = []\n",
    "# grouped_data_w.index\n",
    "for i in np.sort(list(common_total_datapoints)):\n",
    "    # Extract data based on sleep stage\n",
    "    W_data = grouped_data_w.loc[(\"W\",i)].tolist()\n",
    "    _1_data = grouped_data_1.loc[(\"1\",i)].tolist()\n",
    "    _2_data = grouped_data_2.loc[(\"2\",i)].tolist()\n",
    "    \n",
    "    get_box_plot(W_data,_1_data ,_2_data,i,run_freq_w[i],run_freq_1[i],run_freq_2[i])\n",
    "    # all_figures.append(fig)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots per subject for all datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_box_plot(W_data,_1_data ,_2_data,datapoints,samples_w,samples_1,samples_2):\n",
    "\n",
    "    # Combine NREM1 and NREM2 data for one of the boxplots\n",
    "    sleep_data = _1_data + _2_data\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # Plot boxplots for Wake vs Sleep\n",
    "    plt.subplot(2, 1, 1)\n",
    "    boxplot = plt.boxplot([W_data, sleep_data], labels=['Wake', 'Sleep'], \n",
    "                        showmeans=True, showfliers=False, meanline=True, sym='.')\n",
    "\n",
    "    # Customize mean lines\n",
    "    for mean in boxplot['means']:\n",
    "        mean.set_color('green')\n",
    "        mean.set_linewidth(2)\n",
    "        mean.set_linestyle('--')\n",
    "        x, y = mean.get_xdata(), mean.get_ydata()\n",
    "        mean_value = y[0]\n",
    "        plt.text(x[0], mean_value, f'mean: {mean_value:.2f}', \n",
    "                va='bottom', ha='left', color='green')  # Annotate mean value\n",
    "\n",
    "    plt.xlabel('Subject State')\n",
    "    plt.ylabel('Average Brain Hurst Exponent')\n",
    "    plt.title(f'Box Plot of Average Hurst exponent by Wake vs Sleep for {datapoints}')\n",
    "    plt.legend([f'Wake: {samples_w} samples , Sleep: {samples_1+samples_2} samples'])\n",
    "\n",
    "    # Plot boxplots for Wake vs NREM1 vs NREM2\n",
    "    plt.subplot(2, 1, 2)\n",
    "    boxplot = plt.boxplot([W_data, _1_data, _2_data], labels=['Wake', 'NREM1', 'NREM2'], \n",
    "                        showmeans=True, showfliers=False, meanline=True, sym='.')\n",
    "\n",
    "    # Customize mean lines\n",
    "    for mean in boxplot['means']:\n",
    "        mean.set_color('green')\n",
    "        mean.set_linewidth(2)\n",
    "        mean.set_linestyle('--')\n",
    "        x, y = mean.get_xdata(), mean.get_ydata()\n",
    "        mean_value = y[0]\n",
    "        plt.text(x[0], mean_value, f'mean: {mean_value:.2f}', \n",
    "                va='bottom', ha='left', color='green')  # Annotate mean value\n",
    "\n",
    "    plt.xlabel('Sleep Score')\n",
    "    plt.ylabel('Average Hurst Exponent')\n",
    "    plt.title(f'Box Plot of Average Hurst exponent by Wake vs NREM1 vs NREM2 for {datapoints}')\n",
    "    plt.legend([f'Wake: {samples_w} samples , NREM1: {samples_1} samples, NREM2: {samples_2} samples'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the data\n",
    "file_path = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_corrected.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Get all unique values of 'Total datapoints' column\n",
    "total_datapoints_values = df['subject'].unique()\n",
    "\n",
    "columns_to_consider = [df.columns[0]] +[df.columns[3]] + list(df.columns[5:])\n",
    "\n",
    "# Group the DataFrame by 'sleep_stage' and 'Total datapoints'\n",
    "grouped_data_w = df[df['sleep_stage'] == 'W'].groupby(['subject'])\n",
    "grouped_data_1 = df[df['sleep_stage'] == '1'].groupby(['subject'])\n",
    "grouped_data_2 = df[df['sleep_stage'] == '2'].groupby(['subject'])\n",
    "\n",
    "run_freq_w = grouped_data_w['subject'].value_counts().to_dict()\n",
    "run_freq_1 = grouped_data_1['subject'].value_counts().to_dict()\n",
    "run_freq_2 = grouped_data_2['subject'].value_counts().to_dict()\n",
    "\n",
    "grouped_data_w = df[df['sleep_stage'] == 'W'][columns_to_consider].groupby([\"sleep_stage\",\"subject\"]).mean()\n",
    "grouped_data_1 = df[df['sleep_stage'] == '1'][columns_to_consider].groupby(['sleep_stage', 'subject']).mean()\n",
    "grouped_data_2 = df[df['sleep_stage'] == '2'][columns_to_consider].groupby(['sleep_stage', 'subject']).mean()\n",
    "\n",
    "\n",
    "# Filter the DataFrame for sleep stage 'W'\n",
    "W_data = grouped_data_w.reset_index()\n",
    "W_total_datapoints = W_data['subject'].unique()\n",
    "\n",
    "# Filter the DataFrame for sleep stage '1'\n",
    "_1_data = grouped_data_1.reset_index()\n",
    "_1_total_datapoints = _1_data['subject'].unique()\n",
    "\n",
    "# Filter the DataFrame for sleep stage '2'\n",
    "_2_data = grouped_data_2.reset_index()\n",
    "_2_total_datapoints = _2_data['subject'].unique()\n",
    "\n",
    "# Compare the 'Total datapoints' values\n",
    "common_total_datapoints = set(W_total_datapoints) & set(_1_total_datapoints) & set(_2_total_datapoints)\n",
    "\n",
    "all_figures = []\n",
    "# grouped_data_w.index\n",
    "for i in np.sort(list(common_total_datapoints)):\n",
    "    # Extract data based on sleep stage\n",
    "    W_data = grouped_data_w.loc[(\"W\",i)].tolist()\n",
    "    _1_data = grouped_data_1.loc[(\"1\",i)].tolist()\n",
    "    _2_data = grouped_data_2.loc[(\"2\",i)].tolist()\n",
    "    \n",
    "    get_box_plot(W_data,_1_data ,_2_data,i,run_freq_w[i],run_freq_1[i],run_freq_2[i])\n",
    "    # all_figures.append(fig)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots based on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='../output_directory/evaluation_dataset/dfa_hurst_dataset_roi.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Timeline = T1\n",
    "W_data_1 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-rest_run-1')]['AWB'].tolist()\n",
    "N1_data_1 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-rest_run-1')]['AWB'].tolist()\n",
    "N2_data_1 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-rest_run-1')]['AWB'].tolist()\n",
    "data_1 = W_data_1 + N1_data_1 + N2_data_1\n",
    "\n",
    "#Timeline = T2\n",
    "W_data_2 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-rest_run-2')]['AWB'].tolist()\n",
    "N1_data_2 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-rest_run-2')]['AWB'].tolist()\n",
    "N2_data_2 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-rest_run-2')]['AWB'].tolist()\n",
    "data_2 = W_data_2 + N1_data_2 + N2_data_2\n",
    "\n",
    "#Timeline = T3\n",
    "W_data_3 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-1')]['AWB'].tolist()\n",
    "N1_data_3 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-1')]['AWB'].tolist()\n",
    "N2_data_3 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-1')]['AWB'].tolist()\n",
    "data_3 = W_data_3 + N1_data_3 + N2_data_3\n",
    "\n",
    "#Timeline = T4\n",
    "W_data_4 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-2')]['AWB'].tolist()\n",
    "N1_data_4 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-2')]['AWB'].tolist()\n",
    "N2_data_4 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-2')]['AWB'].tolist()\n",
    "data_4 = W_data_4 + N1_data_4 + N2_data_4\n",
    "\n",
    "#Timeline = T5\n",
    "W_data_5 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-3')]['AWB'].tolist()\n",
    "N1_data_5 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-3')]['AWB'].tolist()\n",
    "N2_data_5 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-3')]['AWB'].tolist()\n",
    "data_5 = W_data_5 + N1_data_5 + N2_data_5\n",
    "\n",
    "#Timeline = T6\n",
    "W_data_6 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-3')]['AWB'].tolist()\n",
    "N1_data_6 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-3')]['AWB'].tolist()\n",
    "N2_data_6 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-3')]['AWB'].tolist()\n",
    "data_6 = W_data_6 + N1_data_6 + N2_data_6\n",
    "\n",
    "#Timeline = T7\n",
    "W_data_7 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-4')]['AWB'].tolist()\n",
    "N1_data_7 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-4')]['AWB'].tolist()\n",
    "N2_data_7 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-4')]['AWB'].tolist()\n",
    "data_7 = W_data_7 + N1_data_7 + N2_data_7\n",
    "\n",
    "#Timeline = T8\n",
    "W_data_8 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-5')]['AWB'].tolist()\n",
    "N1_data_8 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-5')]['AWB'].tolist()\n",
    "N2_data_8 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-5')]['AWB'].tolist()\n",
    "data_8 = W_data_8 + N1_data_8 + N2_data_8\n",
    "\n",
    "#Timeline = T9\n",
    "W_data_9 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-6')]['AWB'].tolist()\n",
    "N1_data_9 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-6')]['AWB'].tolist()\n",
    "N2_data_9 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-6')]['AWB'].tolist()\n",
    "data_9 = W_data_9 + N1_data_9 + N2_data_9\n",
    "\n",
    "#Timeline = T10\n",
    "W_data_10 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-7')]['AWB'].tolist()\n",
    "N1_data_10 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-7')]['AWB'].tolist()\n",
    "N2_data_10 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-7')]['AWB'].tolist()\n",
    "data_10 = W_data_10 + N1_data_10 + N2_data_10\n",
    "\n",
    "\n",
    "#Timeline = T11\n",
    "W_data_11 = df[(df['sleep_stage'] == 'W') & (df['run'] == 'task-sleep_run-8')]['AWB'].tolist()\n",
    "N1_data_11 = df[(df['sleep_stage'] == '1') & (df['run'] == 'task-sleep_run-8')]['AWB'].tolist()\n",
    "N2_data_11 = df[(df['sleep_stage'] == '2') & (df['run'] == 'task-sleep_run-8')]['AWB'].tolist()\n",
    "data_11 = W_data_11 + N1_data_11 + N2_data_11\n",
    "\n",
    "median_values=[]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "# boxplot=plt.boxplot([W_data_1, N1_data_1,N2_data_1,W_data_2, N1_data_2,N2_data_2,W_data_3, N1_data_3,N2_data_3,W_data_4, N1_data_4,N2_data_4,W_data_5, N1_data_5,N2_data_5,W_data_6, N1_data_6,N2_data_6,W_data_7, N1_data_7,N2_data_7,W_data_8, N1_data_8,N2_data_8,W_data_9, N1_data_9,N2_data_9,W_data_10, N1_data_10,N2_data_10,W_data_11, N1_data_11,N2_data_11], labels=['T1']*3 + ['T2']*3+ ['T3']*3 + ['T4']*3+['T5']*3 + ['T6']*3+['T7']*3 + ['T8']*3+['T9']*3 + ['T10']*3+['T11']*3,showmeans=False, showfliers=False, sym='.')\n",
    "boxplot=plt.boxplot([data_1,data_2,data_3,data_4,data_5,data_6,data_7,data_8,data_9,data_10,data_11], labels=['T1'] + ['T2']+ ['T3'] + ['T4']+['T5'] + ['T6']+['T7'] + ['T8']+['T9'] + ['T10']+['T11'],showmeans=False, showfliers=False, sym='.')\n",
    "\n",
    "for median in boxplot['medians']:\n",
    "    median.set_color('red')  \n",
    "    median.set_linewidth(2)  \n",
    "    x, y = median.get_xdata(), median.get_ydata()\n",
    "    median_values.append(y[0])  \n",
    "    plt.text(x[0], y[0], f'{y[0]:.2f}', va='bottom', ha='left', color='red') \n",
    "\n",
    "\n",
    "# plt.plot([1, 4,7,10,13,16,19,22,25,28,31], median_values[::3], color='blue', linestyle='--')\n",
    "# plt.plot([2, 5,8,11,14,17,20,23,26,29,32], median_values[1::3], color='green', linestyle='--')\n",
    "# plt.plot([3, 6,9,12,15,18,21,24,27,30,33], median_values[2::3], color='orange', linestyle='--')\n",
    "\n",
    "plt.plot(np.arange(1, 12),median_values, color='blue', linestyle='--')\n",
    "\n",
    "plt.xlabel('Subject State')\n",
    "plt.ylabel('Average Brain Hurst Exponent')\n",
    "plt.title('Box Plot of Average Hurst Exponent vs Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vionlin Plot for showing transition from Wake to Sleep per subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def get_violin_plot(transitions,transitions_melted,parameter_1, parameter_2):\n",
    "    # Create the violin plot with seaborn\n",
    "    sns.violinplot(data=transitions_melted, x='Condition', y='Hurst_Exponent',inner=\"point\",saturation=0.4)\n",
    "\n",
    "    # Draw lines connecting the paired points for each subject\n",
    "    for i in range(len(transitions)):\n",
    "        if transitions.iloc[i][parameter_1] > transitions.iloc[i][parameter_2]:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        \n",
    "        # Plot the line with the determined color\n",
    "        plt.plot([parameter_1, parameter_2], \n",
    "                [transitions.iloc[i][parameter_1], transitions.iloc[i][parameter_2]],\n",
    "                color=color, alpha=1)\n",
    "    plt.ylim(0.4, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_roi_corrected.csv'  # Replace with your actual file path\n",
    "hurst_data = pd.read_csv(file_path)\n",
    "\n",
    "# Averaging across all the regions to obtain a single Hurst exponent for each observation\n",
    "hurst_columns = hurst_data.columns[5:]\n",
    "\n",
    "# Preparing data for wake and sleep states\n",
    "wake_data = hurst_data[hurst_data['sleep_stage'] == 'W']\n",
    "nrem1_data = hurst_data[hurst_data['sleep_stage'] == '1']\n",
    "nrem2_data = hurst_data[hurst_data['sleep_stage'] == '2']\n",
    "sleep_data = pd.concat([nrem1_data,nrem2_data])\n",
    "\n",
    "\n",
    "# Find the common subjects across Wake, NREM1, and NREM2\n",
    "common_subjects = set(wake_data['subject']) & set(sleep_data['subject'])\n",
    "\n",
    "# Filter the original dataframes to only include these common subjects\n",
    "wake_data_common = wake_data[wake_data['subject'].isin(sorted(common_subjects))]\n",
    "sleep_data_common = sleep_data[sleep_data['subject'].isin(sorted(common_subjects))]\n",
    "\n",
    "\n",
    "wake_grouped = wake_data_common.groupby(['Total datapoints',\"subject\"])['AWB'].mean().reset_index()\n",
    "sleep_grouped = sleep_data_common.groupby(['Total datapoints',\"subject\"])['AWB'].mean().reset_index()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 8))\n",
    "# Final touches on the plot\n",
    "plt.title('Transition of Subjects\\' Hurst Exponents from Wake to Sleep')\n",
    "\n",
    "# Merging the wake and sleep dataframes\n",
    "transitions_WS = pd.merge(wake_grouped, sleep_grouped, on='subject', suffixes=('_Wake', '_Sleep'))\n",
    "\n",
    "transitions_WS = transitions_WS[transitions_WS[\"Total datapoints_Wake\"]==transitions_WS[\"Total datapoints_Sleep\"]]\n",
    "# Melt the DataFrame to make it suitable for seaborn's violinplot function\n",
    "transitions_WS_melted = pd.melt(transitions_WS, id_vars=['subject'], value_vars=['AWB_Wake', 'AWB_Sleep'],\n",
    "                             var_name='Condition', value_name='Hurst_Exponent')\n",
    "\n",
    "transitions_WS[\"Difference\"] = transitions_WS[\"AWB_Wake\"] - transitions_WS[\"AWB_Sleep\"]\n",
    "get_violin_plot(transitions_WS,transitions_WS_melted,\"AWB_Wake\", \"AWB_Sleep\")\n",
    "\n",
    "print(common_subjects)\n",
    "transitions_WS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vionlin Plot for showing transition from Wake to NREM 1 and NREM 2 per subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = '../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_roi_corrected.csv'  # Replace with your actual file path\n",
    "hurst_data = pd.read_csv(file_path)\n",
    "\n",
    "# Averaging across all the regions to obtain a single Hurst exponent for each observation\n",
    "hurst_columns = hurst_data.columns[5:]\n",
    "hurst_data['Mean_Hurst_Exponent'] = hurst_data[hurst_columns].mean(axis=1)\n",
    "\n",
    "# Preparing data for wake and sleep states\n",
    "wake_data = hurst_data[hurst_data['sleep_stage'] == 'W']\n",
    "nrem1_data = hurst_data[hurst_data['sleep_stage'] == '1']\n",
    "nrem2_data = hurst_data[hurst_data['sleep_stage'] == '2']\n",
    "sleep_data = pd.concat([nrem1_data,nrem2_data])\n",
    "\n",
    "\n",
    "# Find the common subjects across Wake, NREM1, and NREM2\n",
    "common_subjects = set(wake_data['subject']) & set(nrem1_data['subject']) & set(nrem2_data['subject'])\n",
    "\n",
    "# Filter the original dataframes to only include these common subjects\n",
    "wake_data_common = wake_data[wake_data['subject'].isin(sorted(common_subjects))]\n",
    "nrem1_data_common = nrem1_data[nrem1_data['subject'].isin(sorted(common_subjects))]\n",
    "nrem2_data_common = nrem2_data[nrem2_data['subject'].isin(sorted(common_subjects))]\n",
    "\n",
    "\n",
    "wake_grouped = wake_data_common.groupby(['Total datapoints',\"subject\"])['AWB'].mean().reset_index()\n",
    "nrem1_grouped = nrem1_data_common.groupby(['Total datapoints',\"subject\"])['AWB'].mean().reset_index()\n",
    "nrem2_grouped = nrem2_data_common.groupby(['Total datapoints',\"subject\"])['AWB'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 8))\n",
    "# Final touches on the plot\n",
    "\n",
    "plt.title('Transition of Subjects\\' Hurst Exponents from Wake to NREM1')\n",
    "\n",
    "# Merging the wake and sleep dataframes\n",
    "transitions_W1 = pd.merge(wake_grouped, nrem1_grouped, on='subject', suffixes=('_Wake', '_NREM1'))\n",
    "transitions_W1 = transitions_W1[transitions_W1[\"Total datapoints_Wake\"]==transitions_W1[\"Total datapoints_NREM1\"]]\n",
    "\n",
    "# Melt the DataFrame to make it suitable for seaborn's violinplot function\n",
    "transitions_W1_melted = pd.melt(transitions_W1, id_vars=['subject'], value_vars=['AWB_Wake', 'AWB_NREM1'],\n",
    "                             var_name='Condition', value_name='Hurst_Exponent')\n",
    "\n",
    "transitions_W1[\"Difference\"] = transitions_W1[\"AWB_Wake\"] - transitions_W1[\"AWB_NREM1\"]\n",
    "\n",
    "get_violin_plot(transitions_W1,transitions_W1_melted,\"AWB_Wake\", \"AWB_NREM1\")\n",
    "\n",
    "transitions_W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "# Final touches on the plot\n",
    "plt.title('Transition of Subjects\\' Hurst Exponents from Wake to NREM2')\n",
    "# Merging the wake and sleep dataframes\n",
    "transitions_W2 = pd.merge(wake_grouped, nrem2_grouped, on='subject', suffixes=('_Wake', '_NREM2'))\n",
    "transitions_W2 = transitions_W2[transitions_W2[\"Total datapoints_Wake\"]==transitions_W2[\"Total datapoints_NREM2\"]]\n",
    "\n",
    "# Melt the DataFrame to make it suitable for seaborn's violinplot function\n",
    "transitions_W2_melted = pd.melt(transitions_W2, id_vars=['subject'], value_vars=['AWB_Wake', 'AWB_NREM2'],\n",
    "                             var_name='Condition', value_name='Hurst_Exponent')\n",
    "\n",
    "transitions_W2[\"Difference\"] = transitions_W2[\"AWB_Wake\"] - transitions_W2[\"AWB_NREM2\"]\n",
    "\n",
    "get_violin_plot(transitions_W2,transitions_W2_melted,\"AWB_Wake\", \"AWB_NREM2\")\n",
    "\n",
    "transitions_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "# Final touches on the plot\n",
    "plt.title('Transition of Subjects\\' Hurst Exponents from NREM1 to NREM2')\n",
    "# Merging the wake and sleep dataframes\n",
    "transitions_12 = pd.merge(nrem1_grouped, nrem2_grouped, on='subject', suffixes=('_NREM1', '_NREM2'))\n",
    "transitions_12 = transitions_12[transitions_12[\"Total datapoints_NREM1\"]==transitions_12[\"Total datapoints_NREM2\"]]\n",
    "\n",
    "# Melt the DataFrame to make it suitable for seaborn's violinplot function\n",
    "transitions_12_melted = pd.melt(transitions_12, id_vars=['subject'], value_vars=['AWB_NREM1', 'AWB_NREM2'],\n",
    "                             var_name='Condition', value_name='Hurst_Exponent')\n",
    "\n",
    "transitions_12[\"Difference\"] = transitions_12[\"AWB_NREM1\"] - transitions_12[\"AWB_NREM2\"]\n",
    "\n",
    "get_violin_plot(transitions_12,transitions_12_melted,\"AWB_NREM1\", \"AWB_NREM2\")\n",
    "\n",
    "transitions_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "t_stat_WS, p_value_WS = stats.ttest_rel(transitions_WS['AWB_Wake'], transitions_WS['AWB_Sleep'])\n",
    "t_stat_W1, p_value_W1 = stats.ttest_rel(transitions_W1['AWB_Wake'], transitions_W1['AWB_NREM1'])\n",
    "t_stat_W2, p_value_W2 = stats.ttest_rel(transitions_W2['AWB_Wake'], transitions_W2['AWB_NREM2'])\n",
    "t_stat_12, p_value_12 = stats.ttest_rel(transitions_12['AWB_NREM1'], transitions_12['AWB_NREM2'])\n",
    "\n",
    "print(f\"Transition from wake to sleep : T-statistic: {t_stat_WS}, P-value: {p_value_WS}\")\n",
    "print(f\"Transition from wake to NREM1 : T-statistic: {t_stat_W1}, P-value: {p_value_W1}\")\n",
    "print(f\"Transition from wake to NREM2 : T-statistic: {t_stat_W2}, P-value: {p_value_W2}\")\n",
    "print(f\"Transition from NREM1 to NREM2 : T-statistic: {t_stat_12}, P-value: {p_value_12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Testing:\n",
    "    - t-testing for Wake vs Sleep\n",
    "    - Anova test for Wake vs N1 vs N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 't_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mt_test\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_t_test\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manova_test\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_anova_test\n\u001b[1;32m      4\u001b[0m evaluate_t_test(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_roi_corrected.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, fdr_corrected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, save_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 't_test'"
     ]
    }
   ],
   "source": [
    "from t_test import evaluate_t_test\n",
    "from anova_test import evaluate_anova_test\n",
    "\n",
    "evaluate_t_test('../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_roi_corrected.csv', fdr_corrected = False, save_results = False)\n",
    "evaluate_anova_test('../output_directory/evaluation_dataset/hurst_evaluation/dfa_hurst_dataset_roi_corrected.csv', fdr_corrected = False, save_results = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Mixed Effect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('../output_directory/evaluation_dataset/dfa_hurst_dataset_roi.csv')\n",
    "\n",
    "\n",
    "df['sleep_stage'] = df['sleep_stage'].astype('category')\n",
    "df['sleep_stage'] = df['sleep_stage'].cat.reorder_categories(['W', '1', '2', '3'], ordered=True)\n",
    "df = df.rename(columns={col: col.replace(' ', '_') for col in df.columns})\n",
    "\n",
    "formula = 'AWB ~ sleep_stage + run'\n",
    "random_formula = '0 + run'\n",
    "\n",
    "# Fit the Linear Mixed-Effects Model\n",
    "model = smf.mixedlm(formula, data=df, groups=df['sleep_stage'])\n",
    "result = model.fit()\n",
    "\n",
    "# df.head()\n",
    "print(result.summary())\n",
    "\n",
    "# Extract the coefficients from the model results\n",
    "# coefficients = result.params\n",
    "# # print(coefficients)\n",
    "# # Plot the coefficients\n",
    "# plt.figure(figsize=(20,5))\n",
    "# sns.barplot(x=coefficients.index, y=coefficients.values)\n",
    "# plt.xlabel('Coefficient')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Coefficients of Linear Mixed-Effects Model')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "p_values = result.pvalues\n",
    "\n",
    "# Step 2: Prepare the data for plotting\n",
    "predictor_variables = p_values.index\n",
    "\n",
    "# Step 3: Create a plot to visualize the p-values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(predictor_variables, p_values, color='skyblue')\n",
    "plt.xlabel('P-values')\n",
    "plt.ylabel('Predictor Variables')\n",
    "plt.title('P-values of Predictor Variables')\n",
    "plt.axvline(x=0.05, color='red', linestyle='--', linewidth=1)  # Add a line at alpha = 0.05\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest p-values at the top\n",
    "plt.show()\n",
    "\n",
    "\n",
    "predicted_values = result.fittedvalues\n",
    "\n",
    "# Create scatter plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['AWB'], predicted_values, color='blue', alpha=0.5)\n",
    "plt.xlabel('Observed AWB')\n",
    "plt.ylabel('Predicted AWB')\n",
    "plt.title('Observed vs. Predicted AWB')\n",
    "\n",
    "# Add a diagonal line for reference\n",
    "plt.plot([min(df['AWB']), max(df['AWB'])], [min(df['AWB']), max(df['AWB'])], color='red', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connectivity Matrix\n",
    "- Time series \n",
    "- Hurst Exponent\n",
    "- Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "def get_connectivity_matrix(file_path):\n",
    "\n",
    "    # Fetch cortical atlas\n",
    "    cortical_atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "    cortical_maps = cortical_atlas.maps\n",
    "    cortical_labels = cortical_atlas.labels\n",
    "    cortical_masker = maskers.NiftiLabelsMasker(labels_img=cortical_maps, standardize=True)\n",
    "\n",
    "    # Fetch subcortical atlas\n",
    "    subcortical_atlas = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')\n",
    "    subcortical_maps = subcortical_atlas.maps\n",
    "    subcortical_labels = subcortical_atlas.labels\n",
    "    subcortical_masker = maskers.NiftiLabelsMasker(labels_img=subcortical_maps, standardize=True)\n",
    "    \n",
    "    combined_labels = cortical_labels[1:] + subcortical_labels[1:]  # Skip 'Background' label from subcortical labels\n",
    "\n",
    "    # Apply masker\n",
    "    time_series_cortical = cortical_masker.fit_transform(file_path)\n",
    "    time_series_subcortical = subcortical_masker.fit_transform(file_path)\n",
    "    time_series_combined = np.concatenate((time_series_cortical, time_series_subcortical), axis=1)\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    correlation_measure = ConnectivityMeasure(kind=\"correlation\", standardize=\"zscore_sample\")\n",
    "    correlation_matrix = correlation_measure.fit_transform([time_series_combined])[0]\n",
    "\n",
    "    # Set diagonal to zero\n",
    "    np.fill_diagonal(correlation_matrix, 0)\n",
    "\n",
    "    ax = plotting.plot_matrix(correlation_matrix, labels=combined_labels,cmap=plt.cm.RdBu_r, colorbar=True,title=\"Correlation matrix of ROIs with connectivity measure using correlation\")\n",
    "    plt.gcf().set_size_inches(20, 20)\n",
    "    plt.show()\n",
    "\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.imshow(correlation_matrix, cmap='viridis', interpolation='nearest')\n",
    "    # plt.colorbar(label='Correlation strength')\n",
    "    # plt.title('Functional Connectome')\n",
    "    # plt.xlabel('ROIs')\n",
    "    # plt.ylabel('ROIs')\n",
    "    # plt.show()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "get_connectivity_matrix(file_path=\"../output_directory/sub-05/func/sub-05_task-rest_run-2_space-MNI152NLin2009cAsym_desc-preproc_bold_postproc_smooth.nii.gz\")\n",
    "get_connectivity_matrix(file_path=\"../output_directory/sub-03/func/sub-03_task-sleep_run-5_space-MNI152NLin2009cAsym_desc-preproc_bold_postproc_smooth.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def calculate_mean(file_path, parameter_index):\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    columns_to_consider = [df.columns[parameter_index]] + list(df.columns[5:])\n",
    "\n",
    "    # Filter out zero values\n",
    "    non_zero_df = df[df[columns_to_consider] != 0]\n",
    "\n",
    "    # Group by parameter and calculate the mean for each region\n",
    "    mean_non_zero_hurst_exponents = non_zero_df.groupby(df.columns[parameter_index], as_index=False).mean()\n",
    "\n",
    "    return mean_non_zero_hurst_exponents\n",
    "\n",
    "\n",
    "def compute_connectivity(hurst_exponents):\n",
    "    num_regions = len(hurst_exponents)\n",
    "    connectivity_matrix = np.zeros((num_regions, num_regions))\n",
    "    for i in range(num_regions):\n",
    "        for j in range(i + 1, num_regions):  # Only compute upper triangle (excluding diagonal)\n",
    "            connectivity_matrix[i, j] = abs(hurst_exponents[i] - hurst_exponents[j])\n",
    "            connectivity_matrix[j, i] = connectivity_matrix[i, j]  # Symmetric matrix\n",
    "    return connectivity_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurst Connectivity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../output_directory/evaluation_dataset/dfa_hurst_dataset.csv'\n",
    "mean_hurst_per_sleep_stage = calculate_mean(file_path, 3)\n",
    "sleep_stage_dict_cortical = {}\n",
    "sleep_stage_dict_subcortical = {}\n",
    "\n",
    "sleep_scores = ['W','1', '2']\n",
    "\n",
    "for score in sleep_scores:\n",
    "    # Filter the DataFrame for the current sleep score\n",
    "    score_df = mean_hurst_per_sleep_stage[mean_hurst_per_sleep_stage['sleep_stage'] == score]\n",
    "\n",
    "    # Extract connectivity data for the current sleep score\n",
    "    connectivity_data = score_df.iloc[:, 5:].values  # Assuming connectivity data starts from column index 5\n",
    "    hurst_connectivity = compute_connectivity(connectivity_data[0])\n",
    "    # Plot the connectivity matrix\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(hurst_connectivity, cmap='coolwarm', xticklabels=score_df.columns[5:], yticklabels=score_df.columns[5:])\n",
    "    plt.title(f'Connectivity Matrix for hurst exponent Sleep Score {score}')\n",
    "    plt.xlabel('Brain Regions')\n",
    "    plt.ylabel('Brain Regions')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Connectivity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../output_directory/evaluation_dataset/variance_dataset.csv'\n",
    "mean_hurst_per_sleep_stage = calculate_mean(file_path, 3)\n",
    "sleep_stage_dict_cortical = {}\n",
    "sleep_stage_dict_subcortical = {}\n",
    "\n",
    "sleep_scores = ['W','1', '2']\n",
    "\n",
    "for score in sleep_scores:\n",
    "    # Filter the DataFrame for the current sleep score\n",
    "    score_df = mean_hurst_per_sleep_stage[mean_hurst_per_sleep_stage['sleep_stage'] == score]\n",
    "\n",
    "    # Extract connectivity data for the current sleep score\n",
    "    connectivity_data = score_df.iloc[:, 5:].values  # Assuming connectivity data starts from column index 5\n",
    "    hurst_connectivity = compute_connectivity(connectivity_data[0])\n",
    "    # Plot the connectivity matrix\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(hurst_connectivity, cmap='coolwarm', xticklabels=score_df.columns[5:], yticklabels=score_df.columns[5:])\n",
    "    plt.title(f'Connectivity Matrix for Vraiance of Sleep Score {score}')\n",
    "    plt.xlabel('Brain Regions')\n",
    "    plt.ylabel('Brain Regions')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
